{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCr4xKJnhZ6"
   },
   "source": [
    "# Overview\n",
    "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
    "\n",
    "**Grading Policy** \n",
    "Assignment 2 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below.  Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. Feel free to reach out to the TAs and instructors if you have any questions.\n",
    "\n",
    "## Task 1 - Data Collection and Preprocessing (10 points)\n",
    "## Task 1a. Data Loading (5 pts)\n",
    "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
    "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
    "* Randomly sample 12,000 rows.\n",
    "* Split the sampled data into train (10k), development (1k) and test set (1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjieQgrsocnh"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejav7LUqokNc"
   },
   "source": [
    "## Task 1b. Preprocessing (5 pts)\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the following pre-processing steps:\n",
    "  * Lowercase the text\n",
    "  * Remove all punctuation\n",
    "  * tokenize the text \n",
    "*  Build seperate vocabularies for each language. \n",
    "  * Assign each unique word an id value \n",
    "*Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGC-CvmHojdB"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oauhQ1fjsC69"
   },
   "source": [
    "## Task 2. Model Implementation and Training (30 pts)\n",
    "\n",
    "\n",
    "\n",
    "## Task 2a. Encoder-Decoder Model Implementation (10 pts)\n",
    "Implement an Encoder-Decoder model in Pytorch with the following components\n",
    "* A single layer RNN based encoder. \n",
    "* A single layer RNN based decoder\n",
    "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gvR8hz0tMoG"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqdYhxa1uiqF"
   },
   "source": [
    "## Task 2b. Training (10 pts)\n",
    "Implement the code to train the Encoder-Decoder model on the Irish-English data. You will write code for the following:\n",
    "* Training, validation and test dataloaders \n",
    "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cZ-6zHtwkZn"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QofrQ1GAwnDz"
   },
   "source": [
    "# Task 2c. Evaluation on the Test Set (10 pts)\n",
    "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJP145YuxAgq"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_brvXpVJxD7e"
   },
   "source": [
    "## Task 3. Improving NMT using Attention (10 pts) \n",
    "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. In a few sentences explains which model is the best for translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAOUlKtv0MUn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
