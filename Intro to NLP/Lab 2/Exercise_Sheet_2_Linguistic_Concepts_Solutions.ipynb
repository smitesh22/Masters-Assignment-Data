{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RA4X4gQDsFRR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA4X4gQDsFRR"
      },
      "source": [
        "# Exercise Sheet 2 - Linguistic Concepts\n",
        "## Learning Objectives\n",
        "\n",
        "In this sheet we are going to:\n",
        "- learn more about linguistic structure, analysis and data\n",
        "- do a few exercises on basic linguistic concepts\n",
        "- study some functions in `nltk` for linguistic analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjXtNyOtXp3"
      },
      "source": [
        "---\n",
        "# Pen & Paper Exercises\n",
        "# 1. Morphology\n",
        "\n",
        "See lecture slides 16-17.\n",
        "\n",
        "1. What is the stem in the following words: *amusing*, *amusement*, and *amused*.\n",
        "2. What is the lemma in the following words: *amusing*, *amusement*, and *amused*.\n",
        "3. Name two applications that might use these morphological processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHkJpSJvtXp3"
      },
      "source": [
        "1. The stem is **amus**.\n",
        "2. The lemma is **amuse**.\n",
        "3. Query expansion in information retrieval, machine translation, information extraction, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OvRcvqEtXp2"
      },
      "source": [
        "---\n",
        "# 2. Syntax\n",
        "## 2.1. Part-of-Speech Tagging\n",
        "Use the POS tag set from the slides to annotate each word in the following sentence with the correct part of speech (see lecture slides 29-30):\n",
        "\n",
        "> *He had an expensive, but very good lunch at the Thai restaurant with the big windows that is opposite the church.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yyRiJ_FtXp3"
      },
      "source": [
        "**He**`PRP`  **had**`VBD`  **an**`DT`  **expensive**`JJ` **but**`CC`  **very**`RB`  **good**`JJ`  **lunch**`NN`  **at**`IN`  **the**`DT`  **Thai**`JJ/NNP`  **restaurant**`NN`  **with**`IN`  **the**`DET`  **big**`JJ`  **windows**`NNS`  **that**`Wh-Det`  **is**`VBP`  **opposite**`IN/RB`  **the**`DET`  **church**`NN`\n",
        "\n",
        "*Note the ambiguity of “Thai” and “opposite”*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ6xaZCqjU-z"
      },
      "source": [
        "---\n",
        "## 2.2. Context-free Grammars\n",
        "Define a context-free grammar with production rules for terminal and non-terminal symbols that can be used to analyse/generate the following sentence (see lecture slides 34-35):\n",
        "\n",
        "> *He had lunch at the restaurant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01MM0JPJtXpz"
      },
      "source": [
        "**Rules for non-terminal productions**:\n",
        "```\n",
        "S → NP VP\n",
        "NP → N PP\n",
        "NP → DT N\n",
        "NP → PRP\n",
        "VP → V NP\n",
        "PP → P NP\n",
        "```\n",
        "\n",
        "\n",
        "**Rules for terminal productions**:\n",
        "\n",
        "```\n",
        "N → lunch\n",
        "N → restaurant \n",
        "V → had\n",
        "P → at\n",
        "DT → the\n",
        "PRP → he\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymSIg3EbtXp0"
      },
      "source": [
        "---\n",
        "## 2.3. Constituency Parsing\n",
        "Draw the phrase structure for the sentence in **1.** (see lecture slides 35-36)\n",
        "\n",
        "> *He had lunch at the restaurant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bls9hgjk5Jlt"
      },
      "source": [
        "![He had lunch at the restaurant](https://i.imgur.com/yWe1hlN.png)\n",
        "\n",
        "```\n",
        "(S (NP (PRP He)) (VP (V had) (NP (N lunch) (PP (P at) (NP (DT the) (N restaurant))))))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owWlA60KtXp1"
      },
      "source": [
        "---\n",
        "### 2.3.1. Recursive Grammar and Phrase Structure\n",
        "Define the grammar rules and draw the phrase structure for the following sentence (see lecture slide 37-38):\n",
        "\n",
        "> *He had lunch at the restaurant on the corner of the street.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tOi--aetXp2"
      },
      "source": [
        "**Additional Rules for Non-terminals**:\n",
        "```\n",
        "PP → P NP\n",
        "NP → NP PP\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqs3CMfetXp2"
      },
      "source": [
        "![He had lunch at the restaurant on the corner of the street](https://i.imgur.com/1Wf9EsT.png)\n",
        "\n",
        "```\n",
        "(S (NP (PRP He)) (VP (V had) (NP (N lunch) (PP (P at) (NP (NP (DT the) (N restaurant)) (PP (P on) (NP (NP (DT the) (N corner)) (PP (P of) (NP (DT the)(N street))))))))))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVitSnSVtXp4"
      },
      "source": [
        "---\n",
        "## 2.4. Dependency Parsing\n",
        "\n",
        "Draw the dependency structure for the following sentence:\n",
        "\n",
        "> *We ate at a cheap restaurant on the corner of the street.*\n",
        "\n",
        "See lecture slides 40-41 and the Universal Dependencies website for the [list of relations](https://universaldependencies.org/u/dep/) and [annotation guidelines](https://universaldependencies.org/u/overview/syntax.html). You can also encounter another dependency annotation scheme, [Stanford dependencies](https://nlp.stanford.edu/software/dependencies_manual.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btr-ZHhetXp4"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "[Stanford dependencies](https://nlp.stanford.edu/software/dependencies_manual.pdf):\n",
        "\n",
        "![We ate at a cheap restaurant on the corner of the street.](https://i.imgur.com/puv5ZuY.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Universal Dependencies](https://universaldependencies.org/u/dep/) (same parsing, different tree visualisations):\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vvIycG8Ka3LG1jF98zFzayf32v2eMnrY)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1133eAnvt_X9EVcuCdiq20Zs8CjyyKJsi)\n",
        "\n",
        "We used [UDPipe](https://lindat.mff.cuni.cz/services/udpipe/) for parsing and visualisation, and [UD Annotatrix](https://maryszmary.github.io/ud-annotatrix/standalone/annotator.html) for another tree visualisation here."
      ],
      "metadata": {
        "id": "BpZY7gSwyGZi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaflKvaYtXp5"
      },
      "source": [
        "---\n",
        "# NLTK\n",
        "Now we will look at some functions in `nltk` for morphological processing of texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJp5KzfEtXp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbdfaab-feec-4b52-a8b1-6997fdf004b0"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', \"omw-1.4\", 'averaged_perceptron_tagger', 'universal_tagset' ])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYnTrx6tXp7"
      },
      "source": [
        "---\n",
        "# 3. Tokenization\n",
        "\n",
        "Tokenization is the process of dividing a text into smaller units i.e. tokens. \n",
        "\n",
        "`nltk` provides a `word_tokenize` function to tokenize a sentence in a given language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3k_ONfatXp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c21ee0-e10d-48c5-9ed2-0f943dd2f8db"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Galway has a year-round mild, moist, temperate and changeable climate, due to \\\n",
        "the prevailing winds of the North Atlantic Current together with the Gulf Stream.\"\n",
        "\n",
        "tokens = word_tokenize(sentence, language='english')\n",
        "tokens"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Galway',\n",
              " 'has',\n",
              " 'a',\n",
              " 'year-round',\n",
              " 'mild',\n",
              " ',',\n",
              " 'moist',\n",
              " ',',\n",
              " 'temperate',\n",
              " 'and',\n",
              " 'changeable',\n",
              " 'climate',\n",
              " ',',\n",
              " 'due',\n",
              " 'to',\n",
              " 'the',\n",
              " 'prevailing',\n",
              " 'winds',\n",
              " 'of',\n",
              " 'the',\n",
              " 'North',\n",
              " 'Atlantic',\n",
              " 'Current',\n",
              " 'together',\n",
              " 'with',\n",
              " 'the',\n",
              " 'Gulf',\n",
              " 'Stream',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxfz9OiitXp9"
      },
      "source": [
        "`nltk` has many pre-defined tokenizers and you can also create your own. More details about the `tokenize` submodule are available [here](https://www.nltk.org/api/nltk.tokenize.html). \n",
        "\n",
        "The default tokenizer uses two classes, [PunktSentenceTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer) and [TreebankWordTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.treebank.TreebankWordTokenizer) for the tokenization of a document into sentences and words respectively.\n",
        "\n",
        "You can also call `dir` on any Python module to see what attributes (classes and functions) are defined inside it. Uncomment the following the cell to see the attributes in the `tokenize` subpackage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIeij8jctXp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b460d8-ef1f-4a18-fcb4-b33d8caf766c"
      },
      "source": [
        "dir(nltk.tokenize)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'SyllableTokenizer',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordDetokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_treebank_word_tokenizer',\n",
              " 'api',\n",
              " 'blankline_tokenize',\n",
              " 'casual',\n",
              " 'casual_tokenize',\n",
              " 'destructive',\n",
              " 'legality_principle',\n",
              " 'line_tokenize',\n",
              " 'load',\n",
              " 'mwe',\n",
              " 'punkt',\n",
              " 're',\n",
              " 'regexp',\n",
              " 'regexp_span_tokenize',\n",
              " 'regexp_tokenize',\n",
              " 'repp',\n",
              " 'sent_tokenize',\n",
              " 'sexpr',\n",
              " 'sexpr_tokenize',\n",
              " 'simple',\n",
              " 'sonority_sequencing',\n",
              " 'stanford_segmenter',\n",
              " 'string_span_tokenize',\n",
              " 'texttiling',\n",
              " 'toktok',\n",
              " 'treebank',\n",
              " 'util',\n",
              " 'word_tokenize',\n",
              " 'wordpunct_tokenize']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCNTpUnstXp9"
      },
      "source": [
        "Run the following cell to see the details about the `WhitespaceTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtc9Y-eTtXp9"
      },
      "source": [
        "?nltk.tokenize.WhitespaceTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FXWpgXktXp9"
      },
      "source": [
        "Run the following cell to tokenize the same sentence using the `WhitespaceTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG8CYnIttXp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2541292a-3e60-473a-f608-20d0a2661bf2"
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "tokenizer.tokenize(sentence)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Galway',\n",
              " 'has',\n",
              " 'a',\n",
              " 'year-round',\n",
              " 'mild,',\n",
              " 'moist,',\n",
              " 'temperate',\n",
              " 'and',\n",
              " 'changeable',\n",
              " 'climate,',\n",
              " 'due',\n",
              " 'to',\n",
              " 'the',\n",
              " 'prevailing',\n",
              " 'winds',\n",
              " 'of',\n",
              " 'the',\n",
              " 'North',\n",
              " 'Atlantic',\n",
              " 'Current',\n",
              " 'together',\n",
              " 'with',\n",
              " 'the',\n",
              " 'Gulf',\n",
              " 'Stream.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oybiCjGptXp_"
      },
      "source": [
        "Do you notice any differences here compared to the default tokenizer?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGh1m6J6tXp_"
      },
      "source": [
        "Certain types of texts require special considerations for tokenization. For example, hashtags, mentions and emoji in tweets.\n",
        "\n",
        "Tokenize the following tweet using the default tokenizer and TweetTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7l6sEvXtXp_"
      },
      "source": [
        "tweet = '''Dr @OmniaHZayed gives us a smashing overview of #SemanticAnalysis Information Extraction she elucidates how it all work & what are the likely challenges. #wordsensedisambiguation \n",
        "#homonymy \n",
        "#parsing \n",
        "#semanticrolelabelling \n",
        "#NLP'''"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ced4yq-ttXp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319a08b5-f13e-4fd4-cc69-686629756b61"
      },
      "source": [
        "# tokenize using the default tokenizer\n",
        "word_tokenize(tweet)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr',\n",
              " '@',\n",
              " 'OmniaHZayed',\n",
              " 'gives',\n",
              " 'us',\n",
              " 'a',\n",
              " 'smashing',\n",
              " 'overview',\n",
              " 'of',\n",
              " '#',\n",
              " 'SemanticAnalysis',\n",
              " 'Information',\n",
              " 'Extraction',\n",
              " 'she',\n",
              " 'elucidates',\n",
              " 'how',\n",
              " 'it',\n",
              " 'all',\n",
              " 'work',\n",
              " '&',\n",
              " 'what',\n",
              " 'are',\n",
              " 'the',\n",
              " 'likely',\n",
              " 'challenges',\n",
              " '.',\n",
              " '#',\n",
              " 'wordsensedisambiguation',\n",
              " '#',\n",
              " 'homonymy',\n",
              " '#',\n",
              " 'parsing',\n",
              " '#',\n",
              " 'semanticrolelabelling',\n",
              " '#',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok6JbbGjtXqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109a20b5-dfe9-4d79-dfe4-d537cb513711"
      },
      "source": [
        "# tokenize using TweetTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tokenizer.tokenize(tweet)\n",
        "tweet_tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr',\n",
              " '@OmniaHZayed',\n",
              " 'gives',\n",
              " 'us',\n",
              " 'a',\n",
              " 'smashing',\n",
              " 'overview',\n",
              " 'of',\n",
              " '#SemanticAnalysis',\n",
              " 'Information',\n",
              " 'Extraction',\n",
              " 'she',\n",
              " 'elucidates',\n",
              " 'how',\n",
              " 'it',\n",
              " 'all',\n",
              " 'work',\n",
              " '&',\n",
              " 'what',\n",
              " 'are',\n",
              " 'the',\n",
              " 'likely',\n",
              " 'challenges',\n",
              " '.',\n",
              " '#wordsensedisambiguation',\n",
              " '#homonymy',\n",
              " '#parsing',\n",
              " '#semanticrolelabelling',\n",
              " '#NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNhwCaXtXqA"
      },
      "source": [
        "---\n",
        "# 4. Stemming\n",
        "Stemming is a simplified analysis of  word structure by removing endings/beginnings of words - leaving a common stem. There are many algorithms to perform stemming, with [PorterStemmer](https://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer) being one of the most widely used. Run the following cell to see the output from the stemmer for the sentence defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfJxJvDtXqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c56e5f-0618-479e-dec0-f339614a6386"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "for token in tokens:\n",
        "    print(stemmer.stem(token))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "galway\n",
            "ha\n",
            "a\n",
            "year-round\n",
            "mild\n",
            ",\n",
            "moist\n",
            ",\n",
            "temper\n",
            "and\n",
            "changeabl\n",
            "climat\n",
            ",\n",
            "due\n",
            "to\n",
            "the\n",
            "prevail\n",
            "wind\n",
            "of\n",
            "the\n",
            "north\n",
            "atlant\n",
            "current\n",
            "togeth\n",
            "with\n",
            "the\n",
            "gulf\n",
            "stream\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdAVupYRtXqC"
      },
      "source": [
        "---\n",
        "# 5. Lemmatization\n",
        "Lemmatisation is the linguistic analysis of  word structure by a transformation of morphologically related words to a common lemma. \n",
        "\n",
        "Lemmatization is more complex than stemming as it depends on correctly identifying part of speech and meaning of a word in a sentence. Run the following cell to see the output from the [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1AzKSZ7tXqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b27fc32-50ae-4392-f663-5be334cf61f8"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for token in tokens:\n",
        "    print(lemmatizer.lemmatize(token))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Galway\n",
            "ha\n",
            "a\n",
            "year-round\n",
            "mild\n",
            ",\n",
            "moist\n",
            ",\n",
            "temperate\n",
            "and\n",
            "changeable\n",
            "climate\n",
            ",\n",
            "due\n",
            "to\n",
            "the\n",
            "prevailing\n",
            "wind\n",
            "of\n",
            "the\n",
            "North\n",
            "Atlantic\n",
            "Current\n",
            "together\n",
            "with\n",
            "the\n",
            "Gulf\n",
            "Stream\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpDdFxsRtXqF"
      },
      "source": [
        "# 6. Part-of-Speech Tagging\n",
        "\n",
        "Finally, `nltk` provides a `pos_tag` function to annotate a sentence using an *off-the-shelf* tagger which uses the [Penn Treebank tagset](https://www.nltk.org/book/ch05.html) for the English language.\n",
        "\n",
        "**UPenn Tagset**\n",
        "\n",
        "    CC — coordinating conjunction\n",
        "    CD — cardinal digit\n",
        "    DT — determiner\n",
        "    EX — existential there (“there is”, “there exists”)\n",
        "    FW — foreign word\n",
        "    IN — preposition/subordinating conjunction\n",
        "    JJ — adjective (‘big’)\n",
        "    JJR — adjective, comparative (‘bigger’)\n",
        "    JJS — adjective, superlative (‘biggest’)\n",
        "    LS — list marker\n",
        "    MD — modal ('could', 'will')\n",
        "    NN — noun, singular (‘desk’)\n",
        "    NNS — noun plural (‘desks’)\n",
        "    NNP — proper noun, singular (‘Harrison’)\n",
        "    NNPS — proper noun, plural (‘Americans’)\n",
        "    PDT — predeterminer (‘all the kids’)\n",
        "    POS — possessive ending ('parent’s')\n",
        "    PRP — personal pronoun ('I', 'he', 'she')\n",
        "    PRPS — possessive pronoun ('my', 'his', 'hers')\n",
        "    RB — adverb ('very', 'silently')\n",
        "    RBR — adverb, comparative ('better')\n",
        "    RBS — adverb, superlative ('best')\n",
        "    RP — particle ('give up')\n",
        "    TO — to-particle ('to go')\n",
        "    UH — interjection ('errrrrrrrm')\n",
        "    VB — verb, base form ('take')\n",
        "    VBD — verb, past tense ('took')\n",
        "    VBG — verb, gerund/present participle ('taking')\n",
        "    VBN — verb, past participle ('taken')\n",
        "    VBP — verb, sing. present, non-3d ('take')\n",
        "    VBZ — verb, 3rd person sing. present ('takes')\n",
        "    WDT — wh-determiner ('which')\n",
        "    WP — wh-pronoun ('who', 'what')\n",
        "    WP — possessive wh-pronoun ('whose')\n",
        "    WRB — wh-abverb ('where', 'when')\n",
        "\n",
        "There is a [more modern and language independent tagset](http://universaldependencies.org/u/pos/) used within the [Universal Dependencies](http://universaldependencies.org/) framework. Each part of speech also has a set of features (e.g. nouns can have *number, case, gender*; verbs can have *tense, person, number* etc.), which are described [here](https://universaldependencies.org/u/feat/index.html). You can change the default UPenn tagset to UPOS by specifying the `tagset='universal'` in NLTK's `pos_tag` function.\n",
        "\n",
        "**UPOS tagset**\n",
        "\n",
        "    ADJ: adjective\n",
        "    ADP: adposition\n",
        "    ADV: adverb\n",
        "    AUX: auxiliary\n",
        "    CCONJ: coordinating conjunction\n",
        "    DET: determiner\n",
        "    INTJ: interjection\n",
        "    NOUN: noun\n",
        "    NUM: numeral\n",
        "    PART: particle\n",
        "    PRON: pronoun\n",
        "    PROPN: proper noun\n",
        "    PUNCT: punctuation\n",
        "    SCONJ: subordinating conjunction\n",
        "    SYM: symbol\n",
        "    VERB: verb\n",
        "    X: other\n",
        "\n",
        "\n",
        "The `pos_tag` function uses a so-called [PerceptronTagger](http://www.nltk.org/api/nltk.tag.html#nltk.tag.perceptron.PerceptronTagger). The model was trained on on Sections 00-18 of the Wall Street Journal sections of OntoNotes 5. The original implementation comes from Matthew Honnibal (you can read more about it [here](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python)), it outperforms the predecessor maximum entropy POS model in NLTK. \n",
        "\n",
        "\n",
        "You can also create your own tagger which can **learn** from annotated data. This topic will be covered in detail in Lecture 4 on sequence modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdll5WGStXqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5025426b-e888-450b-c95a-d95cf958ad41"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Galway', 'NNP'),\n",
              " ('has', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('year-round', 'JJ'),\n",
              " ('mild', 'NN'),\n",
              " (',', ','),\n",
              " ('moist', 'NN'),\n",
              " (',', ','),\n",
              " ('temperate', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('changeable', 'JJ'),\n",
              " ('climate', 'NN'),\n",
              " (',', ','),\n",
              " ('due', 'JJ'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('prevailing', 'VBG'),\n",
              " ('winds', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('North', 'NNP'),\n",
              " ('Atlantic', 'NNP'),\n",
              " ('Current', 'NNP'),\n",
              " ('together', 'RB'),\n",
              " ('with', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('Gulf', 'NNP'),\n",
              " ('Stream', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(tokens, tagset='universal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te6rzFTLOvpk",
        "outputId": "4bfd0110-60d6-443b-a838-cf1c72a4d696"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Galway', 'NOUN'),\n",
              " ('has', 'VERB'),\n",
              " ('a', 'DET'),\n",
              " ('year-round', 'ADJ'),\n",
              " ('mild', 'NOUN'),\n",
              " (',', '.'),\n",
              " ('moist', 'NOUN'),\n",
              " (',', '.'),\n",
              " ('temperate', 'NOUN'),\n",
              " ('and', 'CONJ'),\n",
              " ('changeable', 'ADJ'),\n",
              " ('climate', 'NOUN'),\n",
              " (',', '.'),\n",
              " ('due', 'ADJ'),\n",
              " ('to', 'PRT'),\n",
              " ('the', 'DET'),\n",
              " ('prevailing', 'VERB'),\n",
              " ('winds', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('North', 'NOUN'),\n",
              " ('Atlantic', 'NOUN'),\n",
              " ('Current', 'NOUN'),\n",
              " ('together', 'ADV'),\n",
              " ('with', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('Gulf', 'NOUN'),\n",
              " ('Stream', 'NOUN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Jyqp5ztXqI"
      },
      "source": [
        "### Can you identify any problems with the tagging of the sentence above?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArSHyx-4tXqI"
      },
      "source": [
        "*mild*, *moist*, and *temperate* are **adjectives** and should have been tagged as **JJ/ADJ**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0gT-yG_tXqJ"
      },
      "source": [
        "---"
      ]
    }
  ]
}