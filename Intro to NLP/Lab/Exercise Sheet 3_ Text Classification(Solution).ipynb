{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMC6LJjMvplO1t+nb5ehRTK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise Sheet 3 - Text Classification"],"metadata":{"id":"B2AElRcqKwD4"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"womLlIO8E3hZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663937205575,"user_tz":-60,"elapsed":3184,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"9a016e60-6ba1-4c3b-e0bf-e7411cc1c47c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import nltk\n","nltk.download(['brown', 'stopwords'])"]},{"cell_type":"code","source":["import nltk.classify.util\n","from nltk.classify import NaiveBayesClassifier\n","from nltk.corpus import brown\n","from nltk.corpus import stopwords\n","# from nltk.tokenize import word_tokenize\n","import string\n","import pandas as pd\n","from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n"],"metadata":{"id":"OGl_skwUF7J2","executionInfo":{"status":"ok","timestamp":1663937206007,"user_tz":-60,"elapsed":434,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\n","# 1. Preprocessing \n","\n","Framework for Machine learning And Feature Extraction: **sklearn**.\n","\n","Classes from sklearn used:\n","1. [sklearn.feature_extraction.text.CountVectorize](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer)\n","  It converts a collection of text documents to a matrix of token counts.\n","\n","2. [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n","  Convert a collection of raw documents to a matrix of TF-IDF features.\n","\n","3. [class sklearn.feature_extraction.text.TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) \n","  Transform a count matrix to a normalized tf or tf-idf representation\n","\n","4. [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). \n","\n","  Build a text report showing the main classification metrics. It shows macro-average, weighted-average and per class scores for `precision`, `recall` and `f1`.\n","It also displays support, which is the actual occurance of the class/label in the dataset.\n","\n","\n","In order to feed the text to `*CountVectorizer`, it needs to exist as sentences. As shown in the following example:\n","\n","| text | label |\n","| ---- | ----- |\n","|The capital expansion programs business firms involve multi-year budgeting true country development programs|government|\n","|Now Dogtown one places creeps marrow worms get old wood veneer|mystery|\n","|This claim submitted District Court dismissed 126 F.Supp.235 alleged violation 7 Clayton Act also 1 2 Sherman Act|government|\n","|Mrs. Meeker struck ready seek anyone's advice least Garth's|\tmystery|\n","|Richmond Va.\t|government|\n","\n","Essentially what we need:\n","\n","X: Array of sentences\n","\n","y: Array of corresponding labels\n","\n","The corpus which we are using is already tokenized. It could be used as it is.\n","But in real life the corpus would rarely be tokenized, so we prepare the data as sentences and labels before proceeding with the exercise.\n","\n","\n","<br>\n","<br>\n","\n","## Tokenization And Detokenisation(instead of `.join()`)\n","\n","The default tokenization method in NLTK involves tokenization using regular expressions as defined in the Penn Treebank (based on English text). It assumes that the text is already split into sentences.\n","\n","This is a very useful form of tokenization since it incorporates several rules of linguistics to split the sentence into the most optimal tokens.\n","\n","Detokenizer is required to put the sentence back together from a list of words, with proper punctuation form."],"metadata":{"id":"F8mCh1CbBsc7"}},{"cell_type":"code","source":["detokenizer = TreebankWordDetokenizer()\n","tokenizer = TreebankWordTokenizer()"],"metadata":{"id":"zjUIAp_ATv7G","executionInfo":{"status":"ok","timestamp":1663937206008,"user_tz":-60,"elapsed":3,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#2. Dataset And Problem Statement\n","\n","## [Brown Corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html)\n","The corpus consists of one million words of American English texts printed in 1961. The texts for the corpus were sampled from 15 different text categories to make the corpus a good standard reference.\n","\n","From this dataset we select two categories:\n","1. government: Text from government documents\n","2. mystery: Text from mystery and detective fiction\n","\n","And we create our own dataset by detokenizing and shuffling the above."],"metadata":{"id":"GXWS03_dJnks"}},{"cell_type":"code","source":["for category in brown.categories():\n","    corpus_length = len(brown.sents(categories=[category]))\n","    print(f'Category: {category:<16}, Dataset Size:{corpus_length}')\n","\n","english_stopwords = stopwords.words('english')\n","punctuations = list(string.punctuation)\n","\n","print('\\n\\nSelecting `government` and `mystery` categories from brown corpus')\n","\n","def filter_and_join(sent_arr, lab):\n","    filtered_tokens = [token for token in sent_arr if (token not in english_stopwords and token not in punctuations)]\n","    return [detokenizer.detokenize(filtered_tokens), lab]\n","\n","## Using the filter_and_join function on all the text inputs of government categories\n","government_text = list(map(lambda x: filter_and_join(x, 'government'), brown.sents(categories=['government'])))\n","\n","## Using the filter_and_join function on all the text inputs of government categories\n","mystery_text = list(map(lambda x: filter_and_join(x, 'mystery'), brown.sents(categories=['mystery'])))\n","\n","dataset = pd.DataFrame(government_text + mystery_text, columns=['text', 'label'])\n","dataset = dataset.sample(frac=1)\n","dataset.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":518},"id":"zohpGXeAGd2A","executionInfo":{"status":"ok","timestamp":1663937215813,"user_tz":-60,"elapsed":9240,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"446aa528-e68c-4f64-babe-e6eed8bf3dde"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Category: adventure       , Dataset Size:4637\n","Category: belles_lettres  , Dataset Size:7209\n","Category: editorial       , Dataset Size:2997\n","Category: fiction         , Dataset Size:4249\n","Category: government      , Dataset Size:3032\n","Category: hobbies         , Dataset Size:4193\n","Category: humor           , Dataset Size:1053\n","Category: learned         , Dataset Size:7734\n","Category: lore            , Dataset Size:4881\n","Category: mystery         , Dataset Size:3886\n","Category: news            , Dataset Size:4623\n","Category: religion        , Dataset Size:1716\n","Category: reviews         , Dataset Size:1751\n","Category: romance         , Dataset Size:4431\n","Category: science_fiction , Dataset Size:948\n","\n","\n","Selecting `government` and `mystery` categories from brown corpus\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                   text       label\n","2620  Commencing death Lucian Sharpe 1899 name Henry...  government\n","4564  She could always predict Stanley going ever si...     mystery\n","6806  I reached hand toward put inside shirt feel he...     mystery\n","2054                                                  3  government\n","4893    Within framework followed strained even macabre     mystery"],"text/html":["\n","  <div id=\"df-8abf31ed-9ea3-4c05-b306-73432b6e2a8f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2620</th>\n","      <td>Commencing death Lucian Sharpe 1899 name Henry...</td>\n","      <td>government</td>\n","    </tr>\n","    <tr>\n","      <th>4564</th>\n","      <td>She could always predict Stanley going ever si...</td>\n","      <td>mystery</td>\n","    </tr>\n","    <tr>\n","      <th>6806</th>\n","      <td>I reached hand toward put inside shirt feel he...</td>\n","      <td>mystery</td>\n","    </tr>\n","    <tr>\n","      <th>2054</th>\n","      <td>3</td>\n","      <td>government</td>\n","    </tr>\n","    <tr>\n","      <th>4893</th>\n","      <td>Within framework followed strained even macabre</td>\n","      <td>mystery</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8abf31ed-9ea3-4c05-b306-73432b6e2a8f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8abf31ed-9ea3-4c05-b306-73432b6e2a8f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8abf31ed-9ea3-4c05-b306-73432b6e2a8f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## PROBLEM STATEMENT\n","\n","Use the given corpus to perform the following tasks:\n","\n","1. Setting Test/Train dataset: Split the dataset in the train and test dataset. (10% test, 90% training)\n","\n","2. Feature Extraction: Use the text to extract the features i.e. Count Vectors and TFIDF.\n","\n","3. Train ML model: Use the extracted Features to train `Naive Bias` models (1 with each extracted feature)\n","\n","4. Evaluation: calculate the precision, recall and f1 score.\n","  Hint: Use classification report\n","\n","5. Inference: Use the given strings and the trained models to predict the class/label of the text.\n","\n","OPTIONAL:\n","Train Any other model of your choice which could do better than the naive bias model."],"metadata":{"id":"u4nuaQAkP1Nr"}},{"cell_type":"markdown","source":["# 3. Split Data into training and testing sets\n","\n"],"metadata":{"id":"Mb2FSGbIGB96"}},{"cell_type":"markdown","source":["## EXERCISE 1\n","Split the dataset in the train and test dataset. The test set should be 10% of the overall dataset size."],"metadata":{"id":"qwLNGNnKU8cQ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_data, test_data =  train_test_split(dataset, test_size=0.1)"],"metadata":{"id":"0__LQnrqSP_T","executionInfo":{"status":"ok","timestamp":1663937215814,"user_tz":-60,"elapsed":17,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# 4. Feature Engineering using raw counts and TF-IDF\n","\n","\n","\n","## Example\n","The vector representation of the text using counts\n"],"metadata":{"id":"H3Fv_VNWUO1O"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","\n","\n","vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer1.fit_transform(corpus)\n","print(X2.toarray())\n","vectorizer1.get_feature_names_out().tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiknmpTXUNtp","executionInfo":{"status":"ok","timestamp":1663937215814,"user_tz":-60,"elapsed":15,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"58a5fcc0-bb9b-4554-977c-3e603a804b30"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n"]},{"output_type":"execute_result","data":{"text/plain":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["- In the example above, method get_feature_names() returns vocabulary of the corpus i.e. number of unique words. \n","- Each document in the corpus is represented with the reference to the vocabulary\n","- Example: In the document 1 i.e. **\"This is the first document.\"** can be rearranged to **[0, \"document\", \"first\", \"is\", 0, 0, \"the\", 0, \"this\"]** which in the end transformed into count vector based on the number of times the given word occurs in the document i.e. **[0 1 1 1 0 0 1 0 1]**\n","\n","\n","\n","Example below shows the vector representation of the text using tf-idf"],"metadata":{"id":"pZ3lQ6HcUhh-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","vectorizer.get_feature_names_out()\n","\n","vectorizer2 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer2.fit_transform(corpus)\n","print(X2.toarray())\n","vectorizer2.get_feature_names_out().tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TowshEhpU4Nq","executionInfo":{"status":"ok","timestamp":1663937215814,"user_tz":-60,"elapsed":13,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"1ef1d2af-901c-48ac-93d4-b717275a79be"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]\n"," [0.         0.6876236  0.         0.28108867 0.         0.53864762\n","  0.28108867 0.         0.28108867]\n"," [0.51184851 0.         0.         0.26710379 0.51184851 0.\n","  0.26710379 0.51184851 0.26710379]\n"," [0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"]},{"output_type":"execute_result","data":{"text/plain":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["- Similar to count vector, each index in tf-idf vector represents word in the vocabulary.\n","- Each value represents the L2 normalized tf-idf of the word in the document."],"metadata":{"id":"G4YxJ_OkVVYH"}},{"cell_type":"markdown","source":["## FEATURE EXTRACTION FOR THE DATASET"],"metadata":{"id":"b7LgvWONQ2PJ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","X_train, y_train = train_data[\"text\"], train_data[\"label\"]\n","X_test, y_test = test_data[\"text\"], test_data[\"label\"]\n","\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train) \n","\n","\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"],"metadata":{"id":"w_qvwbKgYDQb","executionInfo":{"status":"ok","timestamp":1663937215815,"user_tz":-60,"elapsed":11,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 2\n","The features for the training set have already been generated. Now, generate the features for the test set.\n","\n","## WARNING: \n","\n","Make sure that you do not change the features based on the test dataset."],"metadata":{"id":"tMQxZmpAQlKv"}},{"cell_type":"code","source":["X_test_counts = count_vect.transform(X_test)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)"],"metadata":{"id":"CvuZcr8xS1Ld","executionInfo":{"status":"ok","timestamp":1663937234263,"user_tz":-60,"elapsed":5,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# 5. Naive Bias Classifier\n","\n","Naive Bayes is a generative classification model.\n","\n","A generative model learns parameters by maximizing the joint probability  ùëÉ(ùëã,ùëå)  through Bayes' rule by learning  ùëÉ(ùëå)  and  ùëÉ(ùëã|ùëå)  (where  ùëã  are features and  ùëå  are labels).\n","\n","Prediction with Naive Bias\n","\n","$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg) = \\frac{P(\\text{label}) \\times P(\\frac{\\text{features}}{\\text{label}})}{P(\\text{features})}$$\n","\n","Assumption that all features are independant modifies the formula to:\n","\n","$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg)= \\frac{P(\\text{label}) * P\\big(\\frac{f_1}{\\text{label}}\\big)*...  * P\\big(\\frac{f_n}{\\text{label}}\\big)}{P(\\text{features})}$$\n"],"metadata":{"id":"FnQA6P-2QGyx"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report"],"metadata":{"id":"g-hy8bXjYn-f","executionInfo":{"status":"ok","timestamp":1663937237239,"user_tz":-60,"elapsed":4,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# 5 Training And Evaluation\n","\n","\n","## 5.1. Navie Bias \n","\n","#### Training the Gaussian Naive Bayes with word counts feature vectors (CountVectorizer)\n"],"metadata":{"id":"u0RtMdYXZA4Z"}},{"cell_type":"code","source":["# Lets train a Gaussian Naive Bayes clasifier using counts \n","NB_classifier_counts = MultinomialNB()\n","NB_classifier_counts.fit(X_train_counts.toarray(), y_train)\n","# evaluation\n","preds = NB_classifier_counts.predict(X_test_counts.toarray())\n","print(classification_report(y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJgn7KY4xVDi","executionInfo":{"status":"ok","timestamp":1663937240789,"user_tz":-60,"elapsed":1908,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"777d3032-629a-413d-915a-a50972be5f22"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","  government       0.97      0.86      0.91       304\n","     mystery       0.90      0.98      0.94       388\n","\n","    accuracy                           0.93       692\n","   macro avg       0.93      0.92      0.92       692\n","weighted avg       0.93      0.93      0.93       692\n","\n"]}]},{"cell_type":"markdown","source":["## EXERCISE 3\n","Train Naive Bayes using TF-IDF vectors "],"metadata":{"id":"QyG3wKBvZoqh"}},{"cell_type":"code","source":["NB_classifier_tfidf = MultinomialNB()\n","NB_classifier_tfidf.fit(X_train_tfidf.toarray(), y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIAAuLHx14IB","executionInfo":{"status":"ok","timestamp":1663937243265,"user_tz":-60,"elapsed":821,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"31a36c31-47ef-426a-a417-64203c879b88"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## EXERCISE 4\n","Evaluate the results on the test set."],"metadata":{"id":"k3LkYR50Twh5"}},{"cell_type":"code","source":["# evaluation\n","preds = NB_classifier_tfidf.predict(X_test_tfidf.toarray())\n","print(classification_report(y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoUES3VTZfaV","executionInfo":{"status":"ok","timestamp":1663937245228,"user_tz":-60,"elapsed":357,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"3cf497b5-9e3d-44b1-85ac-55fbd6e5d737"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","  government       0.97      0.84      0.90       304\n","     mystery       0.89      0.98      0.93       388\n","\n","    accuracy                           0.92       692\n","   macro avg       0.93      0.91      0.92       692\n","weighted avg       0.93      0.92      0.92       692\n","\n"]}]},{"cell_type":"markdown","source":["#6. Random Examples (Tv Reviews from internet)"],"metadata":{"id":"7FxStUIwKAZJ"}},{"cell_type":"code","source":["citizen_info_ireland = '. The Government is chosen by and is collectively responsible to the D√°il. \\\n","                        There must be a minimum of 7 and a maximum of 15 Ministers. \\\n","                        The Taoiseach, the Tanaiste and the Minister for Finance must be members of the D√°il.\\\n","                        It is possible to have 2 Ministers who are members of the Senate but this rarely happens.'\n","gone_girl_review = 'Audience Reviews for Gone Girl ... \\\n","                          Mesmerizing performances, tense atmosphere, unexpected plot twists and turns \\\n","                          of events, this movie is a real crime thriller!'\n","\n","sherlock_bbc_review = 'Dr Watson, a former army doctor, finds himself sharing a flat with Sherlock Holmes, \\\n","                        an eccentric individual with a knack for solving crimes. Together, they take on the most unusual cases.'\n","\n","\n"],"metadata":{"id":"y6_9DxD3Zxwd","executionInfo":{"status":"ok","timestamp":1663937247270,"user_tz":-60,"elapsed":2,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 5.1\n","Predict the labels for the above text, using either of the model trained in exercise 4.\n"],"metadata":{"id":"CZ9e4q32UUPP"}},{"cell_type":"code","source":["# tfidf_features = tfidf_transformer.transform(count_vect_feats)\n","count_vect_feats = count_vect.transform([citizen_info_ireland, gone_girl_review, sherlock_bbc_review])\n","NB_classifier_counts.predict(count_vect_feats.toarray())\n","# NB_classifier_counts.predict_proba(count_vect_feats.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwLCPCUJbFOO","executionInfo":{"status":"ok","timestamp":1663937248311,"user_tz":-60,"elapsed":585,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"c1f91b7e-6030-4852-89c9-816f507a3214"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['government', 'mystery', 'government'], dtype='<U10')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["#5.2 [OPTIONAL] Random Forest Classifier \n","\n","A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."],"metadata":{"id":"1D4aMljRd-Ou"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","random_forest_classifier = RandomForestClassifier(n_jobs=-1)\n","random_forest_classifier.fit(X_train_counts.toarray(), y_train)\n","preds = random_forest_classifier.predict(X_test_counts.toarray())\n","\n","print(classification_report(y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNYIhcIebG4m","executionInfo":{"status":"ok","timestamp":1663937389027,"user_tz":-60,"elapsed":53506,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"35fbb1ac-1bc7-4598-98b0-d4bf32cccf82"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","  government       0.87      0.87      0.87       304\n","     mystery       0.90      0.90      0.90       388\n","\n","    accuracy                           0.89       692\n","   macro avg       0.88      0.88      0.88       692\n","weighted avg       0.89      0.89      0.89       692\n","\n"]}]},{"cell_type":"code","source":["print(random_forest_classifier.predict(count_vect_feats.toarray()))\n","print(random_forest_classifier.predict_proba(count_vect_feats.toarray()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1KPZsQbeWn_","executionInfo":{"status":"ok","timestamp":1663937389027,"user_tz":-60,"elapsed":20,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"e1ffbec2-9265-4e50-d746-8c98c2d14167"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['government' 'mystery' 'mystery']\n","[[0.77 0.23]\n"," [0.44 0.56]\n"," [0.46 0.54]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"o91aroYYY4Zz","executionInfo":{"status":"aborted","timestamp":1663937216327,"user_tz":-60,"elapsed":4,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":null,"outputs":[]}]}