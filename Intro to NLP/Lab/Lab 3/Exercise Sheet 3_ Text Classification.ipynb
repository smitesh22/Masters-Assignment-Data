{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOHvrhp8s+OdOfllvYnt3me"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise Sheet 3 - Text Classification"],"metadata":{"id":"B2AElRcqKwD4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"womLlIO8E3hZ"},"outputs":[],"source":["import nltk\n","nltk.download(['brown', 'stopwords'])"]},{"cell_type":"code","source":["import nltk.classify.util\n","from nltk.classify import NaiveBayesClassifier\n","from nltk.corpus import brown\n","from nltk.corpus import stopwords\n","# from nltk.tokenize import word_tokenize\n","import string\n","import pandas as pd\n","from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n"],"metadata":{"id":"OGl_skwUF7J2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 1. Preprocessing \n","\n","Framework for Machine learning And Feature Extraction: **sklearn**.\n","\n","Classes from sklearn used:\n","1. [sklearn.feature_extraction.text.CountVectorize](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer)\n","  It converts a collection of text documents to a matrix of token counts.\n","\n","2. [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n","  Convert a collection of raw documents to a matrix of TF-IDF features.\n","\n","3. [class sklearn.feature_extraction.text.TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) \n","  Transform a count matrix to a normalized tf or tf-idf representation\n","\n","4. [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). \n","\n","  Build a text report showing the main classification metrics. It shows macro-average, weighted-average and per class scores for `precision`, `recall` and `f1`.\n","It also displays support, which is the actual occurance of the class/label in the dataset.\n","\n","\n","In order to feed the text to `*CountVectorizer`, it needs to exist as sentences. As shown in the following example:\n","\n","| text | label |\n","| ---- | ----- |\n","|The capital expansion programs business firms involve multi-year budgeting true country development programs|government|\n","|Now Dogtown one places creeps marrow worms get old wood veneer|mystery|\n","|This claim submitted District Court dismissed 126 F.Supp.235 alleged violation 7 Clayton Act also 1 2 Sherman Act|government|\n","|Mrs. Meeker struck ready seek anyone's advice least Garth's|\tmystery|\n","|Richmond Va.\t|government|\n","\n","Essentially what we need:\n","\n","X: Array of sentences\n","\n","y: Array of corresponding labels\n","\n","The corpus which we are using is already tokenized. It could be used as it is.\n","But in real life the corpus would rarely be tokenized, so we prepare the data as sentences and labels before proceeding with the exercise.\n","\n","\n","<br>\n","<br>\n","\n","## Tokenization And Detokenisation(instead of `.join()`)\n","\n","The default tokenization method in NLTK involves tokenization using regular expressions as defined in the Penn Treebank (based on English text). It assumes that the text is already split into sentences.\n","\n","This is a very useful form of tokenization since it incorporates several rules of linguistics to split the sentence into the most optimal tokens.\n","\n","Detokenizer is required to put the sentence back together from a list of words, with proper punctuation form."],"metadata":{"id":"F8mCh1CbBsc7"}},{"cell_type":"code","source":["detokenizer = TreebankWordDetokenizer()\n","tokenizer = TreebankWordTokenizer()"],"metadata":{"id":"zjUIAp_ATv7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. Dataset And Problem Statement\n","\n","## [Brown Corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html)\n","The corpus consists of one million words of American English texts printed in 1961. The texts for the corpus were sampled from 15 different text categories to make the corpus a good standard reference.\n","\n","From this dataset we select two categories:\n","1. government: Text from government documents\n","2. mystery: Text from mystery and detective fiction\n","\n","And we create our own dataset by detokenizing and shuffling the above."],"metadata":{"id":"GXWS03_dJnks"}},{"cell_type":"code","source":["for category in brown.categories():\n","    corpus_length = len(brown.sents(categories=[category]))\n","    print(f'Category: {category:<16}, Dataset Size:{corpus_length}')\n","\n","english_stopwords = stopwords.words('english')\n","punctuations = list(string.punctuation)\n","\n","print('\\n\\nSelecting `government` and `mystery` categories from brown corpus')\n","\n","def filter_and_join(sent_arr, lab):\n","    filtered_tokens = [token for token in sent_arr if (token not in english_stopwords and token not in punctuations)]\n","    return [detokenizer.detokenize(filtered_tokens), lab]\n","\n","## Using the filter_and_join function on all the text inputs of government categories\n","government_text = list(map(lambda x: filter_and_join(x, 'government'), brown.sents(categories=['government'])))\n","\n","## Using the filter_and_join function on all the text inputs of government categories\n","mystery_text = list(map(lambda x: filter_and_join(x, 'mystery'), brown.sents(categories=['mystery'])))\n","\n","dataset = pd.DataFrame(government_text + mystery_text, columns=['text', 'label'])\n","dataset = dataset.sample(frac=1)\n","dataset.head()\n"],"metadata":{"id":"zohpGXeAGd2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PROBLEM STATEMENT\n","\n","Use the given corpus to perform the following tasks:\n","\n","1. Setting Test/Train dataset: Split the dataset in the train and test dataset. (10% test, 90% training)\n","\n","2. Feature Extraction: Use the text to extract the features i.e. Count Vectors and TFIDF.\n","\n","3. Train ML model: Use the extracted Features to train `Naive Bias` models (1 with each extracted feature)\n","\n","4. Evaluation: calculate the precision, recall and f1 score.\n","  Hint: Use classification report\n","\n","5. Inference: Use the given strings and the trained models to predict the class/label of the text.\n","\n","OPTIONAL:\n","Train Any other model of your choice which could do better than the naive bias model."],"metadata":{"id":"u4nuaQAkP1Nr"}},{"cell_type":"markdown","source":["# 3. Split Data into training and testing sets\n","\n"],"metadata":{"id":"Mb2FSGbIGB96"}},{"cell_type":"markdown","source":["## EXERCISE 1\n","Split the dataset in the train and test dataset. The test set should be 10% of the overall dataset size."],"metadata":{"id":"qwLNGNnKU8cQ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_data, test_data =  ## YOUR CODE GOES HERE"],"metadata":{"id":"0__LQnrqSP_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<Details>\n","<summary>HINT</summary>\n","Use the function\n","\n","```python\n","train_test_split(dataset, test_size=???)\n","```\n","\n","</Details>"],"metadata":{"id":"53KFI8kYZkMN"}},{"cell_type":"markdown","source":["# 4. Feature Engineering using raw counts and TF-IDF\n","\n","\n","\n","## Example\n","The vector representation of the text using counts\n"],"metadata":{"id":"H3Fv_VNWUO1O"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","\n","\n","vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer1.fit_transform(corpus)\n","print(X2.toarray())\n","vectorizer1.get_feature_names_out().tolist()"],"metadata":{"id":"tiknmpTXUNtp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- In the example above, method get_feature_names() returns vocabulary of the corpus i.e. number of unique words. \n","- Each document in the corpus is represented with the reference to the vocabulary\n","- Example: In the document 1 i.e. **\"This is the first document.\"** can be rearranged to **[0, \"document\", \"first\", \"is\", 0, 0, \"the\", 0, \"this\"]** which in the end transformed into count vector based on the number of times the given word occurs in the document i.e. **[0 1 1 1 0 0 1 0 1]**\n","\n","\n","\n","Example below shows the vector representation of the text using tf-idf"],"metadata":{"id":"pZ3lQ6HcUhh-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","vectorizer.get_feature_names_out()\n","\n","vectorizer2 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer2.fit_transform(corpus)\n","print(X2.toarray())\n","vectorizer2.get_feature_names_out().tolist()"],"metadata":{"id":"TowshEhpU4Nq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Similar to count vector, each index in tf-idf vector represents word in the vocabulary.\n","- Each value represents the L2 normalized tf-idf of the word in the document."],"metadata":{"id":"G4YxJ_OkVVYH"}},{"cell_type":"markdown","source":["## FEATURE EXTRACTION FOR THE DATASET"],"metadata":{"id":"b7LgvWONQ2PJ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","X_train, y_train = train_data[\"text\"], train_data[\"label\"]\n","X_test, y_test = test_data[\"text\"], test_data[\"label\"]\n","\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train) \n","\n","\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"],"metadata":{"id":"w_qvwbKgYDQb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 2\n","The features for the training set have already been generated. Now, generate the features for the test set.\n","\n","## WARNING: \n","\n","Make sure that you do not change the features based on the test dataset."],"metadata":{"id":"tMQxZmpAQlKv"}},{"cell_type":"code","source":["X_test_counts = ### YOUR CODE GOES HERE\n","X_test_tfidf = ### YOUR CODE GOES HERE"],"metadata":{"id":"CvuZcr8xS1Ld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<Details>\n","<summary>HINT</summary>\n","Use the function\n","\n","Do NOT use the `fit_transform` function as shown above, it is likely to add more features from the testing data, which is not the purpose of the test dataset.\n","\n","</Details>"],"metadata":{"id":"bHIuake2drZ0"}},{"cell_type":"markdown","source":["# 5. Naive Bias Classifier\n","\n","Naive Bayes is a generative classification model.\n","\n","A generative model learns parameters by maximizing the joint probability  ùëÉ(ùëã,ùëå)  through Bayes' rule by learning  ùëÉ(ùëå)  and  ùëÉ(ùëã|ùëå)  (where  ùëã  are features and  ùëå  are labels).\n","\n","Prediction with Naive Bias\n","\n","$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg) = \\frac{P(\\text{label}) \\times P(\\frac{\\text{features}}{\\text{label}})}{P(\\text{features})}$$\n","\n","Assumption that all features are independant modifies the formula to:\n","\n","$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg)= \\frac{P(\\text{label}) * P\\big(\\frac{f_1}{\\text{label}}\\big)*...  * P\\big(\\frac{f_n}{\\text{label}}\\big)}{P(\\text{features})}$$\n"],"metadata":{"id":"FnQA6P-2QGyx"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report"],"metadata":{"id":"g-hy8bXjYn-f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 Training And Evaluation\n","\n","\n","## 5.1. Navie Bias \n","\n","#### Training the Gaussian Naive Bayes with word counts feature vectors (CountVectorizer)\n"],"metadata":{"id":"u0RtMdYXZA4Z"}},{"cell_type":"code","source":["# Lets train a Gaussian Naive Bayes clasifier using counts \n","NB_classifier_counts = MultinomialNB()\n","NB_classifier_counts.fit(X_train_counts.toarray(), y_train)\n","# evaluation\n","preds = NB_classifier_counts.predict(X_test_counts.toarray())\n","print(classification_report(y_test, preds))"],"metadata":{"id":"wJgn7KY4xVDi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 3\n","Train Gaussian Naive Bayes using TF-IDF vectors "],"metadata":{"id":"QyG3wKBvZoqh"}},{"cell_type":"code","source":["NB_classifier_tfidf = ## Your CODE GOES HERE\n","## CODE FOR TRAINING GOES HERE"],"metadata":{"id":"QIAAuLHx14IB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 4\n","Evaluate the results on the test set."],"metadata":{"id":"k3LkYR50Twh5"}},{"cell_type":"code","source":["## CODE FOR EVALUATION GOES HERE"],"metadata":{"id":"YoUES3VTZfaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6. Random Examples (Tv Reviews from internet)"],"metadata":{"id":"7FxStUIwKAZJ"}},{"cell_type":"code","source":["citizen_info_ireland = '. The Government is chosen by and is collectively responsible to the D√°il. \\\n","                        There must be a minimum of 7 and a maximum of 15 Ministers. \\\n","                        The Taoiseach, the Tanaiste and the Minister for Finance must be members of the D√°il.\\\n","                        It is possible to have 2 Ministers who are members of the Senate but this rarely happens.'\n","gone_girl_review = 'Audience Reviews for Gone Girl ... \\\n","                          Mesmerizing performances, tense atmosphere, unexpected plot twists and turns \\\n","                          of events, this movie is a real crime thriller!'\n","\n","sherlock_bbc_review = 'Dr Watson, a former army doctor, finds himself sharing a flat with Sherlock Holmes, \\\n","                        an eccentric individual with a knack for solving crimes. Together, they take on the most unusual cases.'\n","\n","\n"],"metadata":{"id":"y6_9DxD3Zxwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXERCISE 5.1\n","Predict the labels for the above text, using either of the model trained in exercise 4.\n"],"metadata":{"id":"CZ9e4q32UUPP"}},{"cell_type":"code","source":["# YOUR CODE GOES HERE"],"metadata":{"id":"JwLCPCUJbFOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<Details>\n","<summary>HINT</summary>\n","This step is somewhat like the step that is performed on the test set.\n","But keep in mind that the feature extraction step is still to be performed before the prediction step.\n","\n","</Details>"],"metadata":{"id":"kODkEouYh1qd"}},{"cell_type":"markdown","source":["# [OPTIONAL] Exercise\n","## 5.2 Train a Classifier of your choice which performs better than the previous one"],"metadata":{"id":"1D4aMljRd-Ou"}},{"cell_type":"code","source":["## YOUR CODE GOES HERE"],"metadata":{"id":"XNYIhcIebG4m"},"execution_count":null,"outputs":[]}]}