{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  5 |              5 |          10 |\n",
    "| 2    |                  - |             10 |          10 |\n",
    "| 3    |                  5 |              5 |          10 |\n",
    "| 4    |                  5 |              5 |          10 |\n",
    "| 5    |                  5 |              5 |          10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  670k  100  670k    0     0  3390k      0 --:--:-- --:--:-- --:--:-- 3418k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  168k  100  168k    0     0  1567k      0 --:--:-- --:--:-- --:--:-- 1586k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  208k  100  208k    0     0  1039k      0 --:--:-- --:--:-- --:--:-- 1045k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "train_df = train_df.sample(frac= 1)\n",
    "test_df = test_df.sample(frac= 1)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing (10 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "Data preprocessing is a important step that needs to be carried out before building our machine learning models. Data pre-processing and feature engineering ensures data quality which affects our models ability to learn; therefore, it is extremely important that we preprocess our input data before training our model.\n",
    "\n",
    "Data preprocessing techiniques used in for this task are:\n",
    ">1. Tokenization\n",
    ">2. Stopword Removal\n",
    ">3. Removal of Special Characters\n",
    ">4. Stemming\n",
    ">5. Lemmatization\n",
    ">6. Word Embeddings or Word Vectorization\n",
    "\n",
    "1. Tokenization : The input data is in form of text. We cannot pass sentences directly to TF-IDF and get a vector matrix. We need to pass the words as tokens that can be used later to generate TF-IDF matrix.\n",
    "\n",
    "2. Stopword Removal : Stop-words are commonly used words in textual data that don't hold any literal importance like (is, the, an, a). As these words have high term frequency and they don't add any value they are discarded from the feature vector.\n",
    "\n",
    "3. Removal of Special Characters: Special characters are not that useful as a input to a naaive bayes classifier as sentence construct is not taken under consideration. Hence, we can get rid of special characters in this case.\n",
    "\n",
    "4. Stemming : Stemming is the process of transforming a word in the input to its stem form. Stem is basically the word without any suffixes or prefixes.\n",
    "\n",
    "    Example : learned -> learn\n",
    "            flying -> fly\n",
    "\n",
    "5. Lemmatization : Stemming removes the stem from stem from the word but it doesn't get the root word for irregular verbs like taught. Lemmatization is a pre-processing technique that transforms a word to its root word rather than stem word\n",
    "\n",
    "    Example : taught -> teach\n",
    "            sought -> seek\n",
    "            \n",
    "6. Word Vectorization : Word vectorization is a techinque used in NLP to assign real values to word vectors based on their frequencies or any other metrics. For this task we are using TF-IDF (Term Frequency - Inverse Document Frequency) to generate word vectors.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "import gensim\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "remove_stopword = lambda tokens: [token for token in tokens if token not in stopwords]\n",
    "remove_punctuation = lambda tokens: [token.lower() for token in tokens if token not in punctuation]\n",
    "stem_word = lambda words : [stemmer.stem(word) for word in words]\n",
    "lem_word = lambda words : [lemma.lemmatize(word) for word in words]\n",
    "\n",
    "def preprocess(text):\n",
    "    return gensim.utils.simple_preprocess(text)\n",
    "\n",
    "for idx, data in enumerate(train_texts):\n",
    "    train_texts[idx] = detokenizer.detokenize(preprocess(data))\n",
    "    if train_texts[idx] == \"\":\n",
    "        train_texts.pop(idx)\n",
    "        train_labels.pop(idx)\n",
    "    \n",
    "for idx, data in enumerate(test_texts):\n",
    "    test_texts[idx] = detokenizer.detokenize(preprocess(data))\n",
    "    if test_texts[idx] == \"\":\n",
    "        test_texts.pop(idx)\n",
    "        test_labels.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts =  [text.replace(\"_\",\"\") for text in train_texts]\n",
    "test_texts =  [text.replace(\"_\",\"\") for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5937961595273265"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3,3))\n",
    "X_train_counts = count_vect.fit_transform(train_texts) \n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_tfidf.toarray(), train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "X_test_counts = count_vect.transform(test_texts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)    \n",
    "\n",
    "# save your predictions on the test set into this list\n",
    "\n",
    "# ... your code goes here\n",
    "predictions = NB_classifier.predict(X_test_tfidf.toarray())\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "\n",
    "accuracy(test_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics (10 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['label'].value_counts().plot(kind = \"bar\", color = (\"red\", \"blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df['label'].value_counts().plot(kind = \"bar\", color = (\"red\", \"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs it can be concured that dataset for training and testing is imbalanced. Now accuracy score is a poor evaluation metric against imbalanced dataset. \n",
    "\n",
    "Suppose our model predicts all of our testing cases as 0 or 'not a suggestion' it will give 0.75 as accuracy score. Hence, we need better evaluation metrics that acccuracy score. Below are some of the evaluation metrics that we can use.\n",
    "\n",
    "Confustion Matrix : Confusion matrix is a table that lists all the observations predicted in four subdivision for two classes data. They are \n",
    "\n",
    "True Positive <- for observations that were predicted as true and are actually true.\n",
    "True Negative <- for observations that were predicted as false and are actually false.\n",
    "False Postive <- for observations that were  predicted as true but are actually false.\n",
    "False Negative <- for observations that were predicted as false but are actually true.\n",
    "\n",
    "Precision : Out of all the positive results, what is the percentage of acutal positive results.\n",
    "\n",
    "Recall: How many positive observations where predicted correctly.\n",
    "\n",
    "F1-Score : It is the harmonic mean of precision and recall, its value lies between 0 and 1 with the 1 been the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True Postive: 223 \t False Poitive: 110 \n",
      " False Negative: 440 \t True Negative: 581 \n",
      "  \n",
      "\n",
      " Precision class 0: 0.33634992458521873 \t Precision class 1: 0.8408104196816208\n",
      "  \n",
      " Recall: 0.6696696696696697 \t Recall class 0: 0.722636815920398\n",
      "  \n",
      "\n",
      " Accurarcy: 0.5937961595273265\n",
      "  \n",
      " F1 Score: 0.44779116465863456\n"
     ]
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "  '''\n",
    "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "  assert len(labels) == len(predictions)\n",
    "\n",
    "  score = 0.0\n",
    "  \n",
    "  #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "  # your code goes here\n",
    "  true_positive = 0\n",
    "  true_negative = 0\n",
    "  false_positive = 0\n",
    "  false_negative = 0\n",
    "\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "        if label == 1 and prediction == 1:\n",
    "            true_positive += 1\n",
    "        elif label == 0 and prediction == 0:\n",
    "            true_negative += 1\n",
    "        elif label == 0 and prediction == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "  accuracy_score = (true_positive + true_negative) / (true_positive + true_negative + false_positive  + false_negative)\n",
    "  precision_pos_class = true_positive / (true_positive + false_positive)\n",
    "  recall_pos_class = true_positive / (true_positive + false_negative)\n",
    "  precision_neg_class = true_negative / (true_negative + false_negative)\n",
    "  recall_neg_class = true_negative / (true_negative + true_positive)\n",
    "  f1 = true_positive/(true_positive + 0.5*(false_positive + false_negative))\n",
    "\n",
    "  score = \"\"\" True Postive: %s \\t False Poitive: %s \\n False Negative: %s \\t True Negative: %s \n",
    "  \\n\\n Precision class 0: %s \\t Precision class 1: %s\n",
    "  \\n Recall: %s \\t Recall class 0: %s\n",
    "  \\n\\n Accurarcy: %s\n",
    "  \\n F1 Score: %s\"\"\"% (true_positive, false_negative, false_positive, true_negative, precision_pos_class, precision_neg_class,\n",
    "                       recall_pos_class, recall_neg_class, accuracy_score, f1)\n",
    "  #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "  return print(score)\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.57      0.68      1021\n",
      "           1       0.34      0.67      0.45       333\n",
      "\n",
      "    accuracy                           0.59      1354\n",
      "   macro avg       0.59      0.62      0.56      1354\n",
      "weighted avg       0.72      0.59      0.62      1354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "As we can see in the above results. the accuracy and F1 score values for our Naive Bayes model are fairly low. To tackle this we are goining to introduce some new features and preprocessing steps.\n",
    "\n",
    "1. N-gram model : N-grams are used in NLP to get a set of cocurring words that can be used as a input vector. In this case, two-gram model was used as input.\n",
    "\n",
    "    For the following sentence: It's likely to rain\n",
    "    Bi-gram model will give following tokens:\n",
    "    1. It's likely\n",
    "    2. Likely to\n",
    "    3. To Rain\n",
    "\n",
    "\n",
    "2. Downsampling : Datasets for both training and testing data are imbalanced as we have more cases of data samples not been suggestions. To make our dataset balanced we are going to use down-sampling. In downsampling we resize our dataset by taking equal samples of data from all the classes provided. \n",
    "\n",
    "After applying the above features as bi-garms and preprocessing the F1 score improves dramatically from 0.4 to 0.99 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(list(zip(train_texts, train_labels)), columns= [\"data\", \"label\"])\n",
    "test_data = pd.DataFrame(list(zip(test_texts, test_labels)), columns= [\"data\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_data = train_data['label'] == 0\n",
    "pos_train_data = train_data['label'] == 1\n",
    "pos_test_data = test_data['label'] == 1\n",
    "neg_test_data = test_data['label'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "pos_data = train_data.loc[pos_train_data]\n",
    "neg_data = train_data.loc[neg_train_data].sample(n= 1334, random_state=100)\n",
    "\n",
    "train_data = pd.concat([pos_data, neg_data])\n",
    "train_data = shuffle(train_data)\n",
    "\n",
    "pos_data = test_data.loc[pos_train_data]\n",
    "neg_data = test_data.loc[neg_train_data].sample(n= 333, random_state=100)\n",
    "\n",
    "test_data = pd.concat([pos_data, neg_data])\n",
    "test_data = shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = train_data[\"data\"].to_list(), train_data[\"label\"].to_list() \n",
    "test_texts, test_labels = train_data[\"data\"].to_list(), train_data[\"label\"].to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3,3))\n",
    "X_train_counts = count_vect.fit_transform(train_texts) \n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "NB_classifier_improv = GaussianNB()\n",
    "NB_classifier_improv.fit(X_train_tfidf.toarray(), train_labels)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "X_test_counts = count_vect.transform(test_texts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predictions = NB_classifier_improv.predict(X_test_tfidf.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True Postive: 1328 \t False Poitive: 0 \n",
      " False Negative: 6 \t True Negative: 1334 \n",
      "  \n",
      "\n",
      " Precision class 0: 0.9955022488755623 \t Precision class 1: 1.0\n",
      "  \n",
      " Recall: 1.0 \t Recall class 0: 0.5011269722013524\n",
      "  \n",
      "\n",
      " Accurarcy: 0.9977511244377811\n",
      "  \n",
      " F1 Score: 0.9977460555972952\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition (10 marks)\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NZrBayoN4A",
    "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unseen = pd.read_csv(\"test_unseen.csv\")\n",
    "test_texts = test_unseen['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(test_texts):\n",
    "    test_texts[idx] = detokenizer.detokenize(preprocess(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(test_texts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "predictions = NB_classifier_improv.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [],
   "source": [
    "StudentID = \"22223696_Patil\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": predictions}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "sub_df.to_csv(f\"{StudentID}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
