{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  5 |              5 |          10 |\n",
    "| 2    |                  - |             10 |          10 |\n",
    "| 3    |                  5 |              5 |          10 |\n",
    "| 4    |                  5 |              5 |          10 |\n",
    "| 5    |                  5 |              5 |          10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 92  670k   92  616k    0     0   568k      0  0:00:01  0:00:01 --:--:--  569k\n",
      "100  670k  100  670k    0     0   561k      0  0:00:01  0:00:01 --:--:--  562k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  168k  100  168k    0     0   463k      0 --:--:-- --:--:-- --:--:--  464k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  208k  100  208k    0     0   363k      0 --:--:-- --:--:-- --:--:--  363k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "train_df = shuffle(train_df)\n",
    "test_df = shuffle(test_df)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing (10 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "import gensim\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "remove_stopword = lambda tokens: [token for token in tokens if token not in stopwords]\n",
    "remove_punctuation = lambda tokens: [token.lower() for token in tokens if token not in punctuation]\n",
    "stem_word = lambda words : [stemmer.stem(word) for word in words]\n",
    "lem_word = lambda words : [lemma.lemmatize(word) for word in words]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    return gensim.utils.simple_preprocess(text)\n",
    "\n",
    "for idx, data in enumerate(train_texts):\n",
    "    train_texts[idx] = detokenizer.detokenize(preprocess(data))\n",
    "    if train_texts[idx] == \"\":\n",
    "        train_texts.pop(idx)\n",
    "        train_labels.pop(idx)\n",
    "    \n",
    "for idx, data in enumerate(test_texts):\n",
    "    test_texts[idx] = detokenizer.detokenize(preprocess(data))\n",
    "    if test_texts[idx] == \"\":\n",
    "        test_texts.pop(idx)\n",
    "        test_labels.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts =  [text.replace(\"_\",\"\") for text in train_texts]\n",
    "test_texts =  [text.replace(\"_\",\"\") for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in train_texts:\n",
    "    for word in sentence.split(\" \"):\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7784342688330872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "\n",
    "count_vec = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_counts = count_vec.fit_transform(train_texts)\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "X_train_tfidf = tf.fit_transform(X_train_counts)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf.toarray(), train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "\n",
    "X_test_counts = count_vec.transform(test_texts)\n",
    "X_test_tfidf = tf.transform(X_test_counts)\n",
    "predictions = nb.predict(X_test_tfidf.toarray())    # save your predictions on the test set into this list\n",
    "prediction_prob_old =  nb.predict_log_proba(X_test_tfidf.toarray())\n",
    "\n",
    "# ... your code goes here\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics (10 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True Postive: 39 \t False Poitive: 294 \n",
      " False Negative: 6 \t True Negative: 1015 \n",
      "  \n",
      "\n",
      " Precision class 0: 0.8666666666666667 \t Precision class 1: 0.7754010695187166\n",
      "  \n",
      " Recall: 0.11711711711711711 \t Recall class 0: 0.9629981024667932\n",
      "  \n",
      "\n",
      " Accurarcy: 0.7784342688330872\n",
      "  \n",
      " F1 Score: 0.20634920634920634\n"
     ]
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "  '''\n",
    "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "  assert len(labels) == len(predictions)\n",
    "\n",
    "  score = 0.0\n",
    "  \n",
    "  #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "  # your code goes here\n",
    "  true_positive = 0\n",
    "  true_negative = 0\n",
    "  false_positive = 0\n",
    "  false_negative = 0\n",
    "\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "        if label == 1 and prediction == 1:\n",
    "            true_positive += 1\n",
    "        elif label == 0 and prediction == 0:\n",
    "            true_negative += 1\n",
    "        elif label == 0 and prediction == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "  accuracy_score = (true_positive + true_negative) / (true_positive + true_negative + false_positive  + false_negative)\n",
    "  precision_pos_class = true_positive / (true_positive + false_positive)\n",
    "  recall_pos_class = true_positive / (true_positive + false_negative)\n",
    "  precision_neg_class = true_negative / (true_negative + false_negative)\n",
    "  recall_neg_class = true_negative / (true_negative + true_positive)\n",
    "  f1 = true_positive/(true_positive + 0.5*(false_positive + false_negative))\n",
    "\n",
    "  score = \"\"\" True Postive: %s \\t False Poitive: %s \\n False Negative: %s \\t True Negative: %s \n",
    "  \\n\\n Precision class 0: %s \\t Precision class 1: %s\n",
    "  \\n Recall: %s \\t Recall class 0: %s\n",
    "  \\n\\n Accurarcy: %s\n",
    "  \\n F1 Score: %s\"\"\"% (true_positive, false_negative, false_positive, true_negative, precision_pos_class, precision_neg_class,\n",
    "                       recall_pos_class, recall_neg_class, accuracy_score, f1)\n",
    "  #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "  return print(score)\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(list(zip(train_texts, train_labels)), columns= [\"data\", \"label\"])\n",
    "test_data = pd.DataFrame(list(zip(test_texts, test_labels)), columns= [\"data\", \"label\"])\n",
    "\n",
    "neg_train_data = train_data['label'] == 0\n",
    "pos_train_data = train_data['label'] == 1\n",
    "pos_test_data = test_data['label'] == 1\n",
    "neg_test_data = test_data['label'] == 0\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pos_data = train_data.loc[pos_train_data]\n",
    "neg_data = train_data.loc[neg_train_data].sample(n= 1334, random_state=100)\n",
    "\n",
    "train_data = pd.concat([pos_data, neg_data])\n",
    "train_data = shuffle(train_data)\n",
    "\n",
    "pos_data = test_data.loc[pos_train_data]\n",
    "neg_data = test_data.loc[neg_train_data]\n",
    "\n",
    "test_data = pd.concat([pos_data, neg_data])\n",
    "test_data = shuffle(train_data)\n",
    "\n",
    "train_texts, train_labels = train_data[\"data\"].to_list(), train_data[\"label\"].to_list() \n",
    "test_texts, test_labels = train_data[\"data\"].to_list(), train_data[\"label\"].to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "tf = CountVectorizer(ngram_range=(2,2))\n",
    "Pos_tag_X_train = [detokenizer.detokenize([t for w,t in nltk.pos_tag(tokenizer.tokenize(text))])  for text in train_texts]\n",
    "Pos_tag_X_test = [detokenizer.detokenize([t for w,t in nltk.pos_tag(tokenizer.tokenize(text))])  for text in test_texts]\n",
    "\n",
    "New_X_train = tf.fit_transform(Pos_tag_X_train)\n",
    "New_X_test = tf.transform(Pos_tag_X_test)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes her\n",
    "nb = MultinomialNB()\n",
    "nb.fit(New_X_train.toarray(), train_labels)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "predictions = nb.predict(New_X_test.toarray())\n",
    "prediction_prob_new = nb.predict_log_proba(New_X_test.toarray())\n",
    "# ... your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True Postive: 1073 \t False Poitive: 261 \n",
      " False Negative: 339 \t True Negative: 995 \n",
      "  \n",
      "\n",
      " Precision class 0: 0.759915014164306 \t Precision class 1: 0.7921974522292994\n",
      "  \n",
      " Recall: 0.8043478260869565 \t Recall class 0: 0.4811411992263056\n",
      "  \n",
      "\n",
      " Accurarcy: 0.775112443778111\n",
      "  \n",
      " F1 Score: 0.7815003641660597\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for new, old in zip(prediction_prob_new, prediction_prob_old):\n",
    "    if (new[0]+old[0])/2 > (new[1]+old[1])/2:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20072/799844594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20072/1912217055.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(labels, predictions)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;31m# check that labels and predictions are of same length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m   \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_prob_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition (10 marks)\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NZrBayoN4A",
    "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [],
   "source": [
    "# Preparing submission for Kaggle\n",
    "\n",
    "\n",
    "StudentID = \"22223696_Patill\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "test_texts = test_unseen['text']\n",
    "\n",
    "for idx, data in enumerate(test_texts):\n",
    "    test_texts[idx] = detokenizer.detokenize(preprocess(data))\n",
    "\n",
    "test_texts =  [text.replace(\"_\",\"\") for text in test_texts]\n",
    "\n",
    "tf_text_unseen = tf.transform(test_texts)\n",
    "                          \n",
    "predictions = nb.predict(tf_text_unseen.toarray())\n",
    "        \n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": predictions}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "sub_df.to_csv(f\"{StudentID}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
