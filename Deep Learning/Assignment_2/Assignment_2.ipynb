{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT5133 / CT5145 Deep Learning (/Online) 2022-2023\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "## James McDermott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student ID(s): 22223696\n",
    "* Student name(s): Smitesh Nitin Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due date**: midnight Sunday 19 March (end Week 10).\n",
    "\n",
    "**Weighting**: 20% of the module.\n",
    "\n",
    "In this assignment the goal is to take advantage of pre-trained NN models to create an embedding with a dataset of movie posters, and demonstrate how to use that embedding.\n",
    "\n",
    "The dataset is provided, along with some skeleton code for loading it.\n",
    "\n",
    "The individual steps to be carried out are specified below, with `### YOUR CODE HERE` markers, together with the number of marks available for each part.\n",
    "\n",
    "* **Topics**: in Part 5 below, students are asked to add some improvement to their models. In general, these improvements will differ between students (or student groups). **The proposed improvement must be notified to the lecturer at least 1 week before submission, and approved by the lecturer**. If working in a group, the two members of the group should not work on different topics in Part 5: they must work on the same topic and submit identical submissions.\n",
    "\n",
    "* Students are not required to work incrementally on the parts. It is ok to do all the work in one day, so long as you abide by the rules on notifying groups and notifying topics.\n",
    "\n",
    "* **Groups**: students may work solo or in a group of two. A student may not work together in a group with any student they have previously worked on a group project with, in this module or any other in the MSc programme. **Groups must be notified to the lecturer in writing before beginning work and at least 1 week before submission.** If working in a group, both students must submit and both submissions must be identical. If working in a group, both students may be asked to explain any aspect of the code in interview (see below), therefore working independently on separate components is not recommended. Any emails concerning the project should be cc-ed to the other group member.\n",
    "\n",
    "* **Libraries**: code can be written in Keras/Tensorflow, or in PyTorch. \n",
    "\n",
    "* **Plagiarism**: students may discuss the assignment together, but you may not look at another student or group's work or allow other students to view yours (other than within a group). You may use snippets of code (eg 1-2 lines) from the internet, **if you provide a citation with URL**. You may also use a longer snippet of code if it is a utility function, again only with citation. You may not use code from the internet to carry out the core of the assignment. You may not use a large language model to generate code.\n",
    "\n",
    "* **Submission**: after completing your work in this Jupyter notebook, submit the notebook both in `.ipynb` and `.pdf` formats. The content should be identical.\n",
    "\n",
    "* **Interviews**: a number of students may be selected for interview, post-submission. The selection will depend on submissions, and random chance may be used also. Interviews will be held in-person (CT5133) or online (CT5145). Interviews will last approximately 10 minutes. The purpose of interviews will be to assess students' understanding of their own submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Credits\n",
    "\n",
    "The original csv file is from: \n",
    "\n",
    "https://www.kaggle.com/datasets/neha1703/movie-genre-from-its-poster\n",
    "\n",
    "I have added the *year* column for convenience.\n",
    "\n",
    "I believe most of the information is originally from the famous MovieLens dataset:\n",
    "\n",
    "* https://grouplens.org/datasets/movielens/\n",
    "* https://movielens.org/\n",
    "\n",
    "However, I'm not clear whether the poster download URLs (Amazon AWS URLs) which are in the csv obtained from the Kaggle URL above are from a MovieLens source, or elsewhere.\n",
    "\n",
    "To create the dataset we are using, I have randomly sampled a small proportion of the URLs in the csv, and downloaded the images. I have removed those which fail to download. Code below also filters out those which are in black and white, ie 1 channel only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "You can add more imports if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist, pdist, squareform # useful for distances in the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smitesh\\anaconda3\\envs\\Main\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "These functions are provided to save you time. You might not need to understand any of the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 1 channel, so we omit it (268, 182) 290031.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 294266.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 30337.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 3626440.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 50192.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 54880.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 57006.jpg\n"
     ]
    }
   ],
   "source": [
    "# walk the directory containing posters and read them in. all are the same shape: (268, 182).\n",
    "# all have 3 channels, with a few exceptions (see below).\n",
    "# each is named <imdbId>.jpg, which will later allow us to get the metadata from the csv.\n",
    "IDs = []\n",
    "images = []\n",
    "for dirname, _, filenames in os.walk('DL_Sample'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            ID = int(filename[:-4])\n",
    "            pathname = os.path.join(dirname, filename)\n",
    "            im = Image.open(pathname)\n",
    "            imnp = np.array(im, dtype=float)\n",
    "            if len(imnp.shape) != 3: # we'll ignore a few black-and-white (1 channel) images\n",
    "                print(\"This is 1 channel, so we omit it\", imnp.shape, filename)\n",
    "                continue # do not add to our list\n",
    "            IDs.append(ID)\n",
    "            images.append(imnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 268, 182, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Animation|Adventure|Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113497</td>\n",
       "      <td>http://www.imdb.com/title/tt113497</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Action|Adventure|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113228</td>\n",
       "      <td>http://www.imdb.com/title/tt113228</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114885</td>\n",
       "      <td>http://www.imdb.com/title/tt114885</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113041</td>\n",
       "      <td>http://www.imdb.com/title/tt113041</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Comedy|Family|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imdbId                           Imdb Link  \\\n",
       "0  114709  http://www.imdb.com/title/tt114709   \n",
       "1  113497  http://www.imdb.com/title/tt113497   \n",
       "2  113228  http://www.imdb.com/title/tt113228   \n",
       "3  114885  http://www.imdb.com/title/tt114885   \n",
       "4  113041  http://www.imdb.com/title/tt113041   \n",
       "\n",
       "                                Title  IMDB Score                       Genre  \\\n",
       "0                    Toy Story (1995)         8.3  Animation|Adventure|Comedy   \n",
       "1                      Jumanji (1995)         6.9     Action|Adventure|Family   \n",
       "2             Grumpier Old Men (1995)         6.6              Comedy|Romance   \n",
       "3            Waiting to Exhale (1995)         5.7        Comedy|Drama|Romance   \n",
       "4  Father of the Bride Part II (1995)         5.9       Comedy|Family|Romance   \n",
       "\n",
       "                                              Poster    Year  \n",
       "0  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "1  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "2  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "3  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "4  https://images-na.ssl-images-amazon.com/images...  1995.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv\n",
    "df = pd.read_csv(\"Movie_Genre_Year_Poster.csv\", encoding=\"ISO-8859-1\", index_col=\"Unnamed: 0\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates(subset=[\"imdbId\"]) # some imdbId values are duplicates - just drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.set_index(\"imdbId\") # the imdbId is a more useful index, eg as in the next cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.loc[IDs] # ... we can now use .loc to take a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape # 1254 rows matches the image data shape above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df4[\"Year\"].values\n",
    "titles = df4[\"Title\"].values\n",
    "\n",
    "assert img_array.shape[0] == years.shape[0] == titles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename):\n",
    "    \"\"\"Convenience function: we can supply an ID or a filename.\n",
    "    We read and return the image in Image format.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(filename) == int:\n",
    "        # assume its an ID, so create filename\n",
    "        filename = f\"DL_Sample/{filename}.jpg\"\n",
    "        \n",
    "    # now we can assume it's a filename, so open and read\n",
    "    im = Image.open(filename)\n",
    "    \n",
    "    return im\n",
    "\n",
    "def imshow(im):\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Create embedding [3 marks]\n",
    "\n",
    "Use a pretrained model, eg as provided by Keras, to create a flat (ie 1D) embedding vector of some size `embedding_size` for each movie poster, and put all of these together into a single tensor of shape `(n_movies, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/40 [==========================>...] - ETA: 19s"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "n_movies = img_array.shape[0]\n",
    "embedding_size = 4 # YOUR CODE HERE\n",
    "X = tf.zeros((n_movies, embedding_size))\n",
    "### YOUR CODE HERE\n",
    "n_movies = np.array(df4.index)\n",
    "\n",
    "processed_images = []\n",
    "\n",
    "for image in img_array:\n",
    "    processed_images.append(preprocess_input(image))\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(VGG16(include_top=False,input_shape = img_array[0].shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, ))\n",
    "model = Model(model.input, model.output)\n",
    "\n",
    "out = model.predict(img_array)\n",
    "\n",
    "X = torch.cat((torch.tensor(out), torch.tensor(n_movies).unsqueeze(dim=1)), dim=1)\n",
    "assert len(X.shape) == 2 # X should be (n_movies, embedding_size)\n",
    "assert X.shape[0] == len(n_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Define a nearest-neighbour function [3 marks]\n",
    "\n",
    "Write a function `def nearest(img, k)` which accepts an image `img`, and returns the `k` movies in the dataset whose posters are most similar to `img` (as measured in the embedding), ranked by similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(img, k):\n",
    "    ### YOUR CODE HERE\n",
    "    #subset image indexes and embeddings\n",
    "    index = X[:, -1]\n",
    "    vector_space = X[:, :4]\n",
    "    \n",
    "    # get the embedding of the image \n",
    "    image_embedding = [vector_space[i] for i, idx in enumerate(index) if img == (int(idx))]\n",
    "    \n",
    "    euclidean_distance = []\n",
    "    #calculate k nearest images to the image embedding\n",
    "    for idx, embeddings in enumerate(vector_space):\n",
    "        distance = math.dist(image_embedding[0], embeddings)\n",
    "        euclidean_distance.append(distance)\n",
    "        \n",
    "    #gettings the index of k embeddings by shortest distance\n",
    "    index_distance = [idx for idx, distance in enumerate(euclidean_distance) if distance in sorted(euclidean_distance)[:k+1]]\n",
    "    \n",
    "    #returning image ids\n",
    "    return [int(idx) for i, idx in enumerate(index) if i in index_distance and idx != img]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Demonstrate your nearest-neighbour function [4 marks]\n",
    "\n",
    "Choose any movie poster. Call this the query poster. Show it, and use your nearest-neighbour function to show the 3 nearest neighbours (excluding the query itself). This means **call** the function you defined above.\n",
    "\n",
    "Write a comment: in what ways are they similar or dissimilar? Do you agree with the choice and the ranking? Why do you think they are close in the embedding? Do you notice, for example, that the nearest neighbours are from a similar era? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "Q_idx = 43313 # YOUR VALUE HERE - DO NOT USE MY VALUE\n",
    "\n",
    "out = k_nearest(Q_idx, 3)\n",
    "\n",
    "imshow(imread(Q_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in out:\n",
    "    imshow(imread(img))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Year regression [5 marks]\n",
    "\n",
    "Let's investigate the last question (\"similar era\") above by running **regression** on the year, ie attempt to predict the year, given the poster. Use a train-test split. Build a suitable Keras neural network model for this, **as a regression head on top of the embedding from Part 1**. Include comments to explain the purpose of each part of the model. It should be possible to make a prediction, given a new poster (not part of the original dataset). Write a short comment on model performance: is it possible to predict the year? Based on this result, are there trends over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "### YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_array[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Improvements [5 marks]\n",
    "\n",
    "Propose a possible improvement. Some ideas are suggested below. The chosen improvement must be notified to the lecturer at least 1 week before submission and **must be approved by the lecturer to avoid duplication with other students**. Compare the performance between your original and your new model (the proposed improvement might not actually improve on model performance -- that is ok). Some marks will be awarded for more interesting / challenging / novel improvements.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "* Try a different pretrained model for creating the embedding\n",
    "* Alternative ways of reducing the pretrained model's output to a flat vector for the embedding\n",
    "* Gather more data (see the csv file for URLs)\n",
    "* Add different architectural details to the regression head\n",
    "* Fine-tuning\n",
    "* Training an end-to-end convnet of your own design (no pretraining)\n",
    "* Improve the embedding by training a multi-headed model, eg predicting both genre and year\n",
    "* Create a good visualisation of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Year'].fillna(1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = np.array(df4['Year'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(VGG16(include_top=False,input_shape = img_array[0].shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, ))\n",
    "model.add(Dense(1, ))\n",
    "model = Model(model.input, model.output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(img_array, year))\n",
    "random.shuffle(c)\n",
    "img_array, year = zip(*c)\n",
    "\n",
    "### 70 20 10 split for training\n",
    "\n",
    "train_size = int(np.ceil(len(img_array)*0.7))\n",
    "test_size = int(np.ceil(len(img_array)*0.2))\n",
    "val_size = int(np.ceil(len(img_array)*0.1))\n",
    "\n",
    "X_train = img_array[:train_size]\n",
    "y_train = year[:train_size]\n",
    "\n",
    "X_test = img_array[train_size+1:train_size + test_size]\n",
    "y_test = year[train_size+1:train_size + test_size]\n",
    "\n",
    "X_val = img_array[train_size + test_size + 1: train_size + test_size + val_size -1]\n",
    "y_val = year[train_size + test_size + 1: train_size + test_size + val_size -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
