{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RA4X4gQDsFRR"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA4X4gQDsFRR"
      },
      "source": [
        "# Exercise Sheet 2 - Linguistic Concepts\n",
        "## Learning Objectives\n",
        "\n",
        "In this sheet we are going to:\n",
        "- learn more about linguistic structure, analysis and data\n",
        "- do a few exercises on basic linguistic concepts\n",
        "- study some functions in `nltk` for linguistic analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjXtNyOtXp3"
      },
      "source": [
        "---\n",
        "# Pen & Paper Exercises\n",
        "# 1. Morphology\n",
        "\n",
        "See lecture slides 16-17.\n",
        "\n",
        "1. What is the stem in the following words: *amusing*, *amusement*, and *amused*.\n",
        "2. What is the lemma in the following words: *amusing*, *amusement*, and *amused*.\n",
        "3. Name two applications that might use these morphological processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OvRcvqEtXp2"
      },
      "source": [
        "---\n",
        "# 2. Syntax\n",
        "## 2.1. Part-of-Speech Tagging\n",
        "Use the POS tag set from the slides to annotate each word in the following sentence with the correct part of speech (see lecture slides 29-30):\n",
        "\n",
        "> *He had an expensive, but very good lunch at the Thai restaurant with the big windows that is opposite the church.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ6xaZCqjU-z"
      },
      "source": [
        "---\n",
        "## 2.2. Context-free Grammars\n",
        "Define a context-free grammar with production rules for terminal and non-terminal symbols that can be used to analyse/generate the following sentence (see lecture slides 34-35):\n",
        "\n",
        "> *He had lunch at the restaurant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymSIg3EbtXp0"
      },
      "source": [
        "---\n",
        "## 2.3. Constituency Parsing\n",
        "Draw the phrase structure for the sentence in **1.** (see lecture slides 35-36)\n",
        "\n",
        "> *He had lunch at the restaurant.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owWlA60KtXp1"
      },
      "source": [
        "---\n",
        "### 2.3.1. Recursive Grammar and Phrase Structure\n",
        "Define the grammar rules and draw the phrase structure for the following sentence (see lecture slide 37-38):\n",
        "\n",
        "> *He had lunch at the restaurant on the corner of the street.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVitSnSVtXp4"
      },
      "source": [
        "---\n",
        "## 2.4. Dependency Parsing\n",
        "\n",
        "Draw the dependency structure for the following sentence:\n",
        "\n",
        "> *We ate at a cheap restaurant on the corner of the street.*\n",
        "\n",
        "See lecture slides 40-41 and the Universal Dependencies website for the [list of relations](https://universaldependencies.org/u/dep/) and [annotation guidelines](https://universaldependencies.org/u/overview/syntax.html). You can also encounter another dependency annotation scheme, [Stanford dependencies](https://nlp.stanford.edu/software/dependencies_manual.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaflKvaYtXp5"
      },
      "source": [
        "---\n",
        "# NLTK\n",
        "Now we will look at some functions in `nltk` for morphological processing of texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJp5KzfEtXp6"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', \"omw-1.4\", 'averaged_perceptron_tagger', 'universal_tagset' ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYnTrx6tXp7"
      },
      "source": [
        "---\n",
        "## 3. Tokenization\n",
        "\n",
        "Tokenization is the process of dividing a text into smaller units i.e. tokens. \n",
        "\n",
        "`nltk` provides a `word_tokenize` function to tokenize a sentence in a given language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3k_ONfatXp7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Galway has a year-round mild, moist, temperate and changeable climate, due to \\\n",
        "the prevailing winds of the North Atlantic Current together with the Gulf Stream.\"\n",
        "\n",
        "tokens = word_tokenize(sentence, language='english')\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxfz9OiitXp9"
      },
      "source": [
        "`nltk` has many pre-defined tokenizers and you can also create your own. More details about the `tokenize` submodule are available [here](https://www.nltk.org/api/nltk.tokenize.html). \n",
        "\n",
        "The default tokenizer uses two classes, [PunktSentenceTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer) and [TreebankWordTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.treebank.TreebankWordTokenizer) for the tokenization of a document into sentences and words respectively.\n",
        "\n",
        "You can also call `dir` on any Python module to see what attributes (classes and functions) are defined inside it. Uncomment the following the cell to see the attributes in the `tokenize` subpackage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIeij8jctXp9"
      },
      "source": [
        "dir(nltk.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCNTpUnstXp9"
      },
      "source": [
        "Run the following cell to see the details about the `WhitespaceTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtc9Y-eTtXp9"
      },
      "source": [
        "?nltk.tokenize.WhitespaceTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FXWpgXktXp9"
      },
      "source": [
        "Run the following cell to tokenize the same sentence using the `WhitespaceTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG8CYnIttXp-"
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "tokenizer.tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oybiCjGptXp_"
      },
      "source": [
        "Do you notice any differences here compared to the default tokenizer?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGh1m6J6tXp_"
      },
      "source": [
        "Certain types of texts require special considerations for tokenization. For example, hashtags, mentions and emoji in tweets.\n",
        "\n",
        "Tokenize the following tweet using the default tokenizer and TweetTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7l6sEvXtXp_"
      },
      "source": [
        "tweet = '''Dr @OmniaHZayed gives us a smashing overview of #SemanticAnalysis Information Extraction she elucidates how it all work & what are the likely challenges. #wordsensedisambiguation \n",
        "#homonymy \n",
        "#parsing \n",
        "#semanticrolelabelling \n",
        "#NLP'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ced4yq-ttXp_"
      },
      "source": [
        "# tokenize using the default tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok6JbbGjtXqA"
      },
      "source": [
        "# tokenize using TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNhwCaXtXqA"
      },
      "source": [
        "---\n",
        "# 4. Stemming\n",
        "Stemming is a simplified analysis of  word structure by removing endings/beginnings of words - leaving a common stem. There are many algorithms to perform stemming, with [PorterStemmer](https://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer) being one of the most widely used. Run the following cell to see the output from the stemmer for the sentence defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfJxJvDtXqB"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "for token in tokens:\n",
        "    print(stemmer.stem(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdAVupYRtXqC"
      },
      "source": [
        "---\n",
        "# 5. Lemmatization\n",
        "Lemmatisation is the linguistic analysis of  word structure by a transformation of morphologically related words to a common lemma. \n",
        "\n",
        "Lemmatization is more complex than stemming as it depends on correctly identifying part of speech and meaning of a word in a sentence. Run the following cell to see the output from the [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1AzKSZ7tXqE"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for token in tokens:\n",
        "    print(lemmatizer.lemmatize(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpDdFxsRtXqF"
      },
      "source": [
        "# 6. Part-of-Speech Tagging\n",
        "\n",
        "Finally, `nltk` provides a `pos_tag` function to annotate a sentence using an *off-the-shelf* tagger which uses the [Penn Treebank tagset](https://www.nltk.org/book/ch05.html) for the English language.\n",
        "\n",
        "**UPenn Tagset**\n",
        "\n",
        "    CC — coordinating conjunction\n",
        "    CD — cardinal digit\n",
        "    DT — determiner\n",
        "    EX — existential there (“there is”, “there exists”)\n",
        "    FW — foreign word\n",
        "    IN — preposition/subordinating conjunction\n",
        "    JJ — adjective (‘big’)\n",
        "    JJR — adjective, comparative (‘bigger’)\n",
        "    JJS — adjective, superlative (‘biggest’)\n",
        "    LS — list marker\n",
        "    MD — modal ('could', 'will')\n",
        "    NN — noun, singular (‘desk’)\n",
        "    NNS — noun plural (‘desks’)\n",
        "    NNP — proper noun, singular (‘Harrison’)\n",
        "    NNPS — proper noun, plural (‘Americans’)\n",
        "    PDT — predeterminer (‘all the kids’)\n",
        "    POS — possessive ending ('parent’s')\n",
        "    PRP — personal pronoun ('I', 'he', 'she')\n",
        "    PRPS — possessive pronoun ('my', 'his', 'hers')\n",
        "    RB — adverb ('very', 'silently')\n",
        "    RBR — adverb, comparative ('better')\n",
        "    RBS — adverb, superlative ('best')\n",
        "    RP — particle ('give up')\n",
        "    TO — to-particle ('to go')\n",
        "    UH — interjection ('errrrrrrrm')\n",
        "    VB — verb, base form ('take')\n",
        "    VBD — verb, past tense ('took')\n",
        "    VBG — verb, gerund/present participle ('taking')\n",
        "    VBN — verb, past participle ('taken')\n",
        "    VBP — verb, sing. present, non-3d ('take')\n",
        "    VBZ — verb, 3rd person sing. present ('takes')\n",
        "    WDT — wh-determiner ('which')\n",
        "    WP — wh-pronoun ('who', 'what')\n",
        "    WP — possessive wh-pronoun ('whose')\n",
        "    WRB — wh-abverb ('where', 'when')\n",
        "\n",
        "There is a [more modern and language independent tagset](http://universaldependencies.org/u/pos/) used within the [Universal Dependencies](http://universaldependencies.org/) framework. Each part of speech also has a set of features (e.g. nouns can have *number, case, gender*; verbs can have *tense, person, number* etc.), which are described [here](https://universaldependencies.org/u/feat/index.html). You can change the default UPenn tagset to UPOS by specifying the `tagset='universal'` in NLTK's `pos_tag` function.\n",
        "\n",
        "**UPOS tagset**\n",
        "\n",
        "    ADJ: adjective\n",
        "    ADP: adposition\n",
        "    ADV: adverb\n",
        "    AUX: auxiliary\n",
        "    CCONJ: coordinating conjunction\n",
        "    DET: determiner\n",
        "    INTJ: interjection\n",
        "    NOUN: noun\n",
        "    NUM: numeral\n",
        "    PART: particle\n",
        "    PRON: pronoun\n",
        "    PROPN: proper noun\n",
        "    PUNCT: punctuation\n",
        "    SCONJ: subordinating conjunction\n",
        "    SYM: symbol\n",
        "    VERB: verb\n",
        "    X: other\n",
        "\n",
        "\n",
        "The `pos_tag` function uses a so-called [PerceptronTagger](http://www.nltk.org/api/nltk.tag.html#nltk.tag.perceptron.PerceptronTagger). The model was trained on on Sections 00-18 of the Wall Street Journal sections of OntoNotes 5. The original implementation comes from Matthew Honnibal (you can read more about it [here](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python)), it outperforms the predecessor maximum entropy POS model in NLTK. \n",
        "\n",
        "\n",
        "You can also create your own tagger which can **learn** from annotated data. This topic will be covered in detail in Lecture 4 on sequence modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdll5WGStXqH"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(tokens, tagset='universal')"
      ],
      "metadata": {
        "id": "Te6rzFTLOvpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Jyqp5ztXqI"
      },
      "source": [
        "### Can you identify any problems with the tagging of the sentence above?"
      ]
    }
  ]
}