{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCr4xKJnhZ6"
   },
   "source": [
    "# Overview\n",
    "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
    "\n",
    "**Grading Policy** \n",
    "Assignment 2 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below.  Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. Feel free to reach out to the TAs and instructors if you have any questions.\n",
    "\n",
    "## Task 1 - Data Collection and Preprocessing (10 points)\n",
    "## Task 1a. Data Loading (5 pts)\n",
    "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
    "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
    "* Randomly sample 12,000 rows.\n",
    "* Split the sampled data into train (10k), development (1k) and test set (1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mjieQgrsocnh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#reading lines from datasets for english and irish data\n",
    "english_data = pd.read_csv(\"DGT-en-ga.txt\\DGT.en-ga.en\", sep = '\\t', names = [\"Englishtext\"])\n",
    "irish_data = pd.read_csv(\"DGT-en-ga.txt\\DGT.en-ga.ga\", sep = '\\t', names = [\"Irishtext\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([english_data, irish_data], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(181143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Irishlen\"] = df[\"Irishtext\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"Englishlen\"] = df[\"Englishtext\"].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Englishtext</th>\n",
       "      <th>Irishtext</th>\n",
       "      <th>Irishlen</th>\n",
       "      <th>Englishlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Procès-verbal of rectification to the Conventi...</td>\n",
       "      <td>Miontuairisc cheartaitheach maidir le Coinbhin...</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Official Journal of the European Union L 147 ...</td>\n",
       "      <td>(Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This rectification has been carried out by mea...</td>\n",
       "      <td>Rinneadh an ceartúchán seo le miontuairisc che...</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On pages 33-34, Annex I:</td>\n",
       "      <td>Ar leathanaigh 33-34, Iarscríbhinn I:</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the entries for the States below are rectified...</td>\n",
       "      <td>maidir leis na hiontrálacha le haghaidh na Stá...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181138</th>\n",
       "      <td>For the Council</td>\n",
       "      <td>Airteagal 2.5 (Coimirce talmhaíochta), agus Ai...</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181139</th>\n",
       "      <td>Position of the European Parliament of 31 Janu...</td>\n",
       "      <td>ciallaíonn ‘idirthréimhse’, i ndáil le hearra ...</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181140</th>\n",
       "      <td>Regulation (EU) No 1305/2013 of the European P...</td>\n",
       "      <td>Airteagal 18 (Coimirciú) d'Iarscríbhinn 2-C ma...</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181141</th>\n",
       "      <td>Regulation (EU) 2017/2393 of the European Parl...</td>\n",
       "      <td>I rith na 10 mbliana tar éis theacht i bhfeidh...</td>\n",
       "      <td>35</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>Regulation (EU) No 1307/2013 of the European P...</td>\n",
       "      <td>nach gcuirfidh an Páirtí eile i bhfeidhm Riala...</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181143 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Englishtext  \\\n",
       "0       Procès-verbal of rectification to the Conventi...   \n",
       "1       (Official Journal of the European Union L 147 ...   \n",
       "2       This rectification has been carried out by mea...   \n",
       "3                                On pages 33-34, Annex I:   \n",
       "4       the entries for the States below are rectified...   \n",
       "...                                                   ...   \n",
       "181138                                    For the Council   \n",
       "181139  Position of the European Parliament of 31 Janu...   \n",
       "181140  Regulation (EU) No 1305/2013 of the European P...   \n",
       "181141  Regulation (EU) 2017/2393 of the European Parl...   \n",
       "181142  Regulation (EU) No 1307/2013 of the European P...   \n",
       "\n",
       "                                                Irishtext  Irishlen  \\\n",
       "0       Miontuairisc cheartaitheach maidir le Coinbhin...        28   \n",
       "1       (Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...        20   \n",
       "2       Rinneadh an ceartúchán seo le miontuairisc che...        28   \n",
       "3                   Ar leathanaigh 33-34, Iarscríbhinn I:         5   \n",
       "4       maidir leis na hiontrálacha le haghaidh na Stá...        14   \n",
       "...                                                   ...       ...   \n",
       "181138  Airteagal 2.5 (Coimirce talmhaíochta), agus Ai...        12   \n",
       "181139  ciallaíonn ‘idirthréimhse’, i ndáil le hearra ...        43   \n",
       "181140  Airteagal 18 (Coimirciú) d'Iarscríbhinn 2-C ma...        10   \n",
       "181141  I rith na 10 mbliana tar éis theacht i bhfeidh...        35   \n",
       "181142  nach gcuirfidh an Páirtí eile i bhfeidhm Riala...        32   \n",
       "\n",
       "        Englishlen  \n",
       "0               27  \n",
       "1               12  \n",
       "2               33  \n",
       "3                5  \n",
       "4               10  \n",
       "...            ...  \n",
       "181138           3  \n",
       "181139          23  \n",
       "181140          37  \n",
       "181141         107  \n",
       "181142          45  \n",
       "\n",
       "[181143 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Irishlen\"] > 28]\n",
    "df = df.sample(12000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejav7LUqokNc"
   },
   "source": [
    "## Task 1b. Preprocessing (5 pts)\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the following pre-processing steps:\n",
    "  * Lowercase the text\n",
    "  * Remove all punctuation\n",
    "  * tokenize the text \n",
    "*  Build seperate vocabularies for each language. \n",
    "  * Assign each unique word an id value \n",
    "*Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGC-CvmHojdB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Your code here\n",
    "class Preprocess:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe        \n",
    "        self.englishwords = []\n",
    "        self.irishwords = []\n",
    "        self.english_valuecounts = {}\n",
    "        self.irish_valuecounts = {}\n",
    "        self.english_dict = {\"PAD\": 0, \"<bof>\": 1, \"<eos>\": 2}\n",
    "        self.english_word2idx = {0: 'PAD', 1: \"<bof>\", 2 : \"<eof>\"}\n",
    "        self.irish_dict = {\"PAD\": 0, \"<bof>\": 1, \"<eos>\": 2}\n",
    "        self.irish_word2idx = {0: 'PAD', 1: \"<bof>\", 2 : \"<eof>\"}\n",
    "        self.english_unique_words = 0\n",
    "        self.irish_unique_words = 0\n",
    "        self.english_word_count = 0\n",
    "        self.irish_word_count = 0\n",
    "        self.encoded_sentence_english = 0\n",
    "        self.encoded_sentence_irish = 0\n",
    "        \n",
    "        \n",
    "    #1. Add '<bof>' to denote beginning of sentence and '<eos>' to denote the end of the sentence to each target line.\n",
    "    def sentenceTags(self):\n",
    "        self.dataframe['Englishtext'] = self.dataframe['Englishtext'].apply(lambda x: '<bof> '+ x + ' <eos>')\n",
    "        self.dataframe['Irishtext'] = self.dataframe['Irishtext'].apply(lambda x: '<bof> '+ x + ' <eos>')\n",
    "        \n",
    "        return self.dataframe\n",
    "    \n",
    "    def preprocess(self):\n",
    "        #Lowercase the text\n",
    "        self.dataframe['Englishtext'] = self.dataframe['Englishtext'].apply(lambda x: str(x).lower())\n",
    "        self.dataframe['Irishtext'] = self.dataframe['Irishtext'].apply(lambda x: x.lower())\n",
    "        \n",
    "        #Remove all punctuation\n",
    "        self.dataframe['Englishtext'] = self.dataframe['Englishtext'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "        self.dataframe['Irishtext'] = self.dataframe['Irishtext'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "        \n",
    "        return self.dataframe\n",
    "        \n",
    "    #tokenize the text\n",
    "    def tokenize(self):\n",
    "        self.dataframe['EnglishTokens'] = self.dataframe['Englishtext'].apply(lambda x: x.split(\" \"))\n",
    "        self.dataframe['IrishTokens'] = self.dataframe['Irishtext'].apply(lambda x: x.split(\" \"))\n",
    "        \n",
    "        return self.dataframe\n",
    "    \n",
    "    def create_dict(self):\n",
    "        self.englishwords = self.dataframe['Englishtext'].str.cat().split(' ')\n",
    "        self.irishwords = self.dataframe['Irishtext'].str.cat().split(' ')\n",
    "        \n",
    "        \n",
    "        #create a word count dictonary\n",
    "        #creat a index to word \n",
    "        value = 3\n",
    "        for word in self.englishwords:\n",
    "            if word not in self.english_dict and word != '':\n",
    "                self.english_dict[word] = value\n",
    "                self.english_word2idx[value] = word\n",
    "                value += 1\n",
    "        value = 3\n",
    "        for word in self.irishwords:\n",
    "            if word not in self.irish_dict and word != '':\n",
    "                self.irish_dict[word] = value\n",
    "                self.irish_word2idx[value] = word\n",
    "                value += 1\n",
    "\n",
    "        \n",
    "        self.english_dict = dict(sorted(self.english_dict.items(), key = lambda x:x[1]))\n",
    "        self.irish_dict = dict(sorted(self.irish_dict.items(), key = lambda x:x[1]))\n",
    "        \n",
    "        \n",
    "        #Assign each unique word an id value \n",
    "        self.english_unique_words = set(self.englishwords)\n",
    "        self.irish_unique_words = set(self.irishwords)\n",
    "        \n",
    "        self.english_word_count = len(self.english_unique_words)\n",
    "        self.irish_word_count = len(self.irish_unique_words)\n",
    "        \n",
    "        print(\"Created dictonaries for indexing unique words and value counts\")\n",
    "        \n",
    "    \n",
    "    def encode_sentence(self):\n",
    "        self.encoded_sentence_english = [[preprocess.english_dict[token] for token in sentence \n",
    "                             if token != ''] for sentence in preprocess.dataframe.EnglishTokens]\n",
    "        self.encoded_sentence_irish = [[preprocess.irish_dict[token] for token in sentence \n",
    "                             if token != ''] for sentence in preprocess.dataframe.IrishTokens]\n",
    "        \n",
    "    \"\"\"\n",
    "        Print statistics on the selected dataset:\n",
    "        Number of samples\n",
    "        Number of unique source language tokens\n",
    "        Number of unique target language tokens\n",
    "        Max sequence length of source language\n",
    "        Max sequence length of target language\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Statistics on dataset\\n\\n\")\n",
    "        \n",
    "        print(\"Number of samples: \"+ str(len(self.dataframe)))\n",
    "        print(\"Unique words in english set: \"+ str(self.english_word_count))\n",
    "        print(\"Unique words in irish set: \"+ str(self.irish_word_count))\n",
    "        print(\"Max sequence length for source language : \"+ str(max(self.dataframe.Englishlen)))\n",
    "        print(\"Max sequence length for target language : \"+ str(max(self.dataframe.Irishlen)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.sentenceTags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.encode_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "def pad_features(english_tokens, irish_tokens):\n",
    "\n",
    "    source = pad_sequences(english_tokens, maxlen = 310, padding = 'post', truncating = 'post', value = 0)\n",
    "    target = pad_sequences(irish_tokens, maxlen = 310, padding = 'post', truncating = 'post', value = 0)\n",
    "        \n",
    "        \n",
    "    return source, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source, data_target = pad_features(preprocess.encoded_sentence_english, preprocess.encoded_sentence_irish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of train source {data_source.shape}, and target {data_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.concat([data_source, data_target])\n",
    "\n",
    "train, test = train_test_split(df, test_size=.3, random_state=42)\n",
    "test, val = train_test_split(test, test_size=.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datasets sizes : Train => \"+ str(len(train)) +\" Test=> \"+ str(len(test)) + \"Validation => \"+ str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_source[0:8000]\n",
    "y_train = data_target[0:8000]\n",
    "\n",
    "X_test = data_source[8000:10000]\n",
    "y_test = data_target[8000:10000]\n",
    "\n",
    "X_val = data_source[10000:]\n",
    "y_val = data_target[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(X_train),\n",
    "        torch.LongTensor(y_train)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = 64\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(X_val),\n",
    "        torch.LongTensor(y_val)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 64\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(X_test),\n",
    "        torch.LongTensor(y_test)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data : \"+str(len(X_train)) + \"   \"+ str(len(y_train)))\n",
    "print(\"Test Data : \"+str(len(X_test)) + \"   \"+ str(len(y_test)))\n",
    "print(\"Validation Data : \"+str(len(X_val)) + \"   \"+ str(len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    print( batch[0].shape, batch[1].shape )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oauhQ1fjsC69"
   },
   "source": [
    "## Task 2. Model Implementation and Training (30 pts)\n",
    "\n",
    "\n",
    "\n",
    "## Task 2a. Encoder-Decoder Model Implementation (10 pts)\n",
    "Implement an Encoder-Decoder model in Pytorch with the following components\n",
    "* A single layer RNN based encoder. \n",
    "* A single layer RNN based decoder\n",
    "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gvR8hz0tMoG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Your code here\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, hidden_dim, encoder_hid_dim, decoder_hid_dim , prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, encoder_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
    "        self.dropout = nn.Dropout(prob)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, hidden_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = target_vocab_size\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "                (enc_hid_dim * 2) + dec_hid_dim + hidden_dim,\n",
    "                target_vocab_size\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedding, weighted), dim = 2)\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        embedding = embedding.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediciton = self.fc_out(torch.cat((output, weighted, embedding), dim = 1))\n",
    "        \n",
    "        return predicitons, hidden.squeeze(0)\n",
    "        \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        target_len = trg.shape[0]\n",
    "        \n",
    "        target_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[0, :]\n",
    "    \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            \n",
    "            best = outputs.argmax(1)\n",
    "            \n",
    "            x = target[1] if random.random() < teacher_forcing_ratio else best\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqdYhxa1uiqF"
   },
   "source": [
    "## Task 2b. Training (10 pts)\n",
    "Implement the code to train the Encoder-Decoder model on the Irish-English data. You will write code for the following:\n",
    "* Training, validation and test dataloaders \n",
    "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cZ-6zHtwkZn"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(preprocess.english_unique_words)\n",
    "OUTPUT_DIM = len(preprocess.irish_unique_words)\n",
    "\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(enc, dec)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        src = batch[0].transpose(1 , 0)\n",
    "        trg = batch[1].transpose(1 , 0)\n",
    "        #trg2 = batch[1].transpose(0 , 1)\n",
    "        print(src.shape,\" :1\")\n",
    "        print(trg.shape,\" :2\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_loss = round(epoch_loss/ len(train_dl), 3)\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in val_dl:\n",
    "        src = batch[0].transpose(1, 0)\n",
    "        trg = batch[1].transpose(1, 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(src, trg)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[-1].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(output, trg)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "    val_loss = round(eval_loss / len(val_dl), 3)\n",
    "    print(f'Epoch {epoch} | train_loss : {train_loss} | train ppl : {np.exp(train_loss)} | val ppl : {np.exp(val_loss)}')\n",
    "    \n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'nest_model.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        src = batch[0].transpose(0, 1)\n",
    "        trg = batch[1].transpose(0, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(output, trg)\n",
    "        \n",
    "        eval_loss += loss.item()\n",
    "    val_loss = round(eval_loss / len(val_dl), 3)\n",
    "    print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(val_loss)}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QofrQ1GAwnDz"
   },
   "source": [
    "# Task 2c. Evaluation on the Test Set (10 pts)\n",
    "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJP145YuxAgq"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_brvXpVJxD7e"
   },
   "source": [
    "## Task 3. Improving NMT using Attention (10 pts) \n",
    "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. In a few sentences explains which model is the best for translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAOUlKtv0MUn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
