{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT5133 / CT5145 Deep Learning (/Online) 2022-2023\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "## James McDermott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student ID(s): 22223696\n",
    "* Student name(s): Smitesh Nitin Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due date**: midnight Sunday 19 March (end Week 10).\n",
    "\n",
    "**Weighting**: 20% of the module.\n",
    "\n",
    "In this assignment the goal is to take advantage of pre-trained NN models to create an embedding with a dataset of movie posters, and demonstrate how to use that embedding.\n",
    "\n",
    "The dataset is provided, along with some skeleton code for loading it.\n",
    "\n",
    "The individual steps to be carried out are specified below, with `### YOUR CODE HERE` markers, together with the number of marks available for each part.\n",
    "\n",
    "* **Topics**: in Part 5 below, students are asked to add some improvement to their models. In general, these improvements will differ between students (or student groups). **The proposed improvement must be notified to the lecturer at least 1 week before submission, and approved by the lecturer**. If working in a group, the two members of the group should not work on different topics in Part 5: they must work on the same topic and submit identical submissions.\n",
    "\n",
    "* Students are not required to work incrementally on the parts. It is ok to do all the work in one day, so long as you abide by the rules on notifying groups and notifying topics.\n",
    "\n",
    "* **Groups**: students may work solo or in a group of two. A student may not work together in a group with any student they have previously worked on a group project with, in this module or any other in the MSc programme. **Groups must be notified to the lecturer in writing before beginning work and at least 1 week before submission.** If working in a group, both students must submit and both submissions must be identical. If working in a group, both students may be asked to explain any aspect of the code in interview (see below), therefore working independently on separate components is not recommended. Any emails concerning the project should be cc-ed to the other group member.\n",
    "\n",
    "* **Libraries**: code can be written in Keras/Tensorflow, or in PyTorch. \n",
    "\n",
    "* **Plagiarism**: students may discuss the assignment together, but you may not look at another student or group's work or allow other students to view yours (other than within a group). You may use snippets of code (eg 1-2 lines) from the internet, **if you provide a citation with URL**. You may also use a longer snippet of code if it is a utility function, again only with citation. You may not use code from the internet to carry out the core of the assignment. You may not use a large language model to generate code.\n",
    "\n",
    "* **Submission**: after completing your work in this Jupyter notebook, submit the notebook both in `.ipynb` and `.pdf` formats. The content should be identical.\n",
    "\n",
    "* **Interviews**: a number of students may be selected for interview, post-submission. The selection will depend on submissions, and random chance may be used also. Interviews will be held in-person (CT5133) or online (CT5145). Interviews will last approximately 10 minutes. The purpose of interviews will be to assess students' understanding of their own submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Credits\n",
    "\n",
    "The original csv file is from: \n",
    "\n",
    "https://www.kaggle.com/datasets/neha1703/movie-genre-from-its-poster\n",
    "\n",
    "I have added the *year* column for convenience.\n",
    "\n",
    "I believe most of the information is originally from the famous MovieLens dataset:\n",
    "\n",
    "* https://grouplens.org/datasets/movielens/\n",
    "* https://movielens.org/\n",
    "\n",
    "However, I'm not clear whether the poster download URLs (Amazon AWS URLs) which are in the csv obtained from the Kaggle URL above are from a MovieLens source, or elsewhere.\n",
    "\n",
    "To create the dataset we are using, I have randomly sampled a small proportion of the URLs in the csv, and downloaded the images. I have removed those which fail to download. Code below also filters out those which are in black and white, ie 1 channel only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "You can add more imports if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist, pdist, squareform # useful for distances in the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smitesh\\anaconda3\\envs\\Main\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet_v2 import ResNet152V2\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Flatten, Dense, Embedding, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Utility functions\n",
    "\n",
    "These functions are provided to save you time. You might not need to understand any of the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 1 channel, so we omit it (268, 182) 290031.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 294266.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 30337.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 3626440.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 50192.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 54880.jpg\n",
      "This is 1 channel, so we omit it (268, 182) 57006.jpg\n"
     ]
    }
   ],
   "source": [
    "# walk the directory containing posters and read them in. all are the same shape: (268, 182).\n",
    "# all have 3 channels, with a few exceptions (see below).\n",
    "# each is named <imdbId>.jpg, which will later allow us to get the metadata from the csv.\n",
    "IDs = []\n",
    "images = []\n",
    "for dirname, _, filenames in os.walk('DL_Sample'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            ID = int(filename[:-4])\n",
    "            pathname = os.path.join(dirname, filename)\n",
    "            im = Image.open(pathname)\n",
    "            imnp = np.array(im, dtype=float)\n",
    "            if len(imnp.shape) != 3: # we'll ignore a few black-and-white (1 channel) images\n",
    "                print(\"This is 1 channel, so we omit it\", imnp.shape, filename)\n",
    "                continue # do not add to our list\n",
    "            IDs.append(ID)\n",
    "            images.append(imnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 268, 182, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Animation|Adventure|Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113497</td>\n",
       "      <td>http://www.imdb.com/title/tt113497</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Action|Adventure|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113228</td>\n",
       "      <td>http://www.imdb.com/title/tt113228</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114885</td>\n",
       "      <td>http://www.imdb.com/title/tt114885</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113041</td>\n",
       "      <td>http://www.imdb.com/title/tt113041</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Comedy|Family|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imdbId                           Imdb Link  \\\n",
       "0  114709  http://www.imdb.com/title/tt114709   \n",
       "1  113497  http://www.imdb.com/title/tt113497   \n",
       "2  113228  http://www.imdb.com/title/tt113228   \n",
       "3  114885  http://www.imdb.com/title/tt114885   \n",
       "4  113041  http://www.imdb.com/title/tt113041   \n",
       "\n",
       "                                Title  IMDB Score                       Genre  \\\n",
       "0                    Toy Story (1995)         8.3  Animation|Adventure|Comedy   \n",
       "1                      Jumanji (1995)         6.9     Action|Adventure|Family   \n",
       "2             Grumpier Old Men (1995)         6.6              Comedy|Romance   \n",
       "3            Waiting to Exhale (1995)         5.7        Comedy|Drama|Romance   \n",
       "4  Father of the Bride Part II (1995)         5.9       Comedy|Family|Romance   \n",
       "\n",
       "                                              Poster    Year  \n",
       "0  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "1  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "2  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "3  https://images-na.ssl-images-amazon.com/images...  1995.0  \n",
       "4  https://images-na.ssl-images-amazon.com/images...  1995.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv\n",
    "df = pd.read_csv(\"Movie_Genre_Year_Poster.csv\", encoding=\"ISO-8859-1\", index_col=\"Unnamed: 0\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates(subset=[\"imdbId\"]) # some imdbId values are duplicates - just drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.set_index(\"imdbId\") # the imdbId is a more useful index, eg as in the next cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.loc[IDs] # ... we can now use .loc to take a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape # 1254 rows matches the image data shape above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df4[\"Year\"].values\n",
    "titles = df4[\"Title\"].values\n",
    "\n",
    "assert img_array.shape[0] == years.shape[0] == titles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename):\n",
    "    \"\"\"Convenience function: we can supply an ID or a filename.\n",
    "    We read and return the image in Image format.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(filename) == int:\n",
    "        # assume its an ID, so create filename\n",
    "        filename = f\"DL_Sample/{filename}.jpg\"\n",
    "        \n",
    "    # now we can assume it's a filename, so open and read\n",
    "    im = Image.open(filename)\n",
    "    \n",
    "    return im\n",
    "\n",
    "def imshow(im):\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Part 1. Create embedding [3 marks]\n",
    "\n",
    "Use a pretrained model, eg as provided by Keras, to create a flat (ie 1D) embedding vector of some size `embedding_size` for each movie poster, and put all of these together into a single tensor of shape `(n_movies, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 7s 0us/step\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 268, 182, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 268, 182, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 268, 182, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 134, 91, 64)       0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 134, 91, 128)      73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 134, 91, 128)      147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 67, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 67, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 67, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 67, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 67, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 33, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 33, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 33, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 33, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 33, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 16, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20480)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      " 4/40 [==>...........................] - ETA: 4:20"
     ]
    }
   ],
   "source": [
    "#initialising processed_image array for storing the preprocessed inputs\n",
    "processed_images = []\n",
    "n_movies = np.array(df4.index)\n",
    "\n",
    "#processing all images before feeding them to network to create embeddings\n",
    "for image in img_array:\n",
    "    #image = image/255.0\n",
    "    processed_images.append(preprocess_input(image))\n",
    "\n",
    "#inilialialising network architecture by removing the dense and output layer \n",
    "#model = VGG16(include_top=False, input_shape= img_array[0].shape)\n",
    "model = VGG19(include_top = False, input_shape = img_array[0].shape, weights=\"imagenet\")\n",
    "\n",
    "#setting the layers as not trainable as we are not training our model just creating embeddings\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# appending a flatten layer that would be the output the embeddings created would be of size of dimension of images\n",
    "x =  model.output\n",
    "predictions = Flatten()(x)\n",
    "\n",
    "# initialising the model with the architecture created\n",
    "model = Model(inputs = model.input, outputs = predictions)\n",
    "model.summary()\n",
    "\n",
    "#creating embeddings\n",
    "out = model.predict(np.array(processed_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating final tensor with the embeddings and their respective IDS\n",
    "X = torch.cat((torch.tensor(out), torch.tensor(IDs).unsqueeze(dim=1)), dim=1)\n",
    "assert len(X.shape) == 2 # X should be (n_movies, embedding_size)\n",
    "assert X.shape[0] == len(n_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Define a nearest-neighbour function [3 marks]\n",
    "\n",
    "Write a function `def nearest(img, k)` which accepts an image `img`, and returns the `k` movies in the dataset whose posters are most similar to `img` (as measured in the embedding), ranked by similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get k posters most similar to the input image\n",
    "def k_nearest(img_id, k):\n",
    "    # getting the index from the previous X tensor\n",
    "    index = X[:, -1]\n",
    "    # getting the embeddings generated\n",
    "    vector_embeddings = X[:, :(len(X[0])-1)]\n",
    "    \n",
    "    #getting the embedding of the input image\n",
    "    image_embedding = [vector_embeddings[i] for i, idx in enumerate(index) if img_id == int(idx)]\n",
    "    #initialising the cosime_similarity list\n",
    "    cosine_similarities = []\n",
    "    #looping through all the embeddings\n",
    "    for idx, embeddings in enumerate(vector_embeddings):\n",
    "        #getting the similarity value between each embedding and image embedding\n",
    "        similarity = cosine_similarity(embeddings.reshape(1,-1), image_embedding[0].reshape(1, -1))\n",
    "        cosine_similarities.append(similarity)\n",
    "    \n",
    "    # getting the idx of images with cosine similarity among top 1:k+1 values\n",
    "    similar_images = [int(idx) for i, idx in enumerate(index) if cosine_similarities[i] in sorted(cosine_similarities, \n",
    "                                                                                                  reverse = True)[1:k+1]]\n",
    "    \n",
    "    print(len(similar_images))\n",
    "    \n",
    "    #orginial image\n",
    "    print(\"Image\")\n",
    "    imshow(imread(img_id))\n",
    "    \n",
    "    #similar images found\n",
    "    print(\"Similar Images\")\n",
    "    for img in similar_images:\n",
    "        imshow(imread(img))\n",
    "    \n",
    "    # score of similar images\n",
    "    print(\"Image Scores: \", sorted(cosine_similarities, reverse = True)[1:k+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Demonstrate your nearest-neighbour function [4 marks]\n",
    "\n",
    "Choose any movie poster. Call this the query poster. Show it, and use your nearest-neighbour function to show the 3 nearest neighbours (excluding the query itself). This means **call** the function you defined above.\n",
    "\n",
    "Write a comment: in what ways are they similar or dissimilar? Do you agree with the choice and the ranking? Why do you think they are close in the embedding? Do you notice, for example, that the nearest neighbours are from a similar era? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR code HERE\n",
    "Q_idx = 17103 # YOUR VALUE HERE - DO NOT USE MY VALUE\n",
    "\n",
    "k_nearest(Q_idx, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Year regression [5 marks]\n",
    "\n",
    "Let's investigate the last question (\"similar era\") above by running **regression** on the year, ie attempt to predict the year, given the poster. Use a train-test split. Build a suitable Keras neural network model for this, **as a regression head on top of the embedding from Part 1**. Include comments to explain the purpose of each part of the model. It should be possible to make a prediction, given a new poster (not part of the original dataset). Write a short comment on model performance: is it possible to predict the year? Based on this result, are there trends over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the index\n",
    "df4 = df4.reset_index()\n",
    "#selecting the title and year\n",
    "df4 = df4[['Title', 'Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if year has any null values\n",
    "df4['Year'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the index of null values\n",
    "index = df4['Year'].index[df4['Year'].apply(np.isnan)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the indexed value that are NaN\n",
    "df4 = df4.drop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embeddings and removing embeddings for which we dont have year data\n",
    "embeddings = [arr for i, arr in enumerate(X[:, :(len(X[0])-1)]) if i not in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the len of img_data and yers are equal\n",
    "assert len(embeddings) == len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending img_data in dataframe for subsetting\n",
    "df4['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# subsetting data into train 70%, test 15%, val 15%\n",
    "train_df, test_df = train_test_split(df4, train_size=0.7, shuffle=True, random_state=1)\n",
    "test_df, val_df = train_test_split(test_df, train_size=0.5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([t.numpy() for t in train_df['embedding']])\n",
    "y_train = np.array([int(t) for t in train_df['Year']])\n",
    "X_val = np.array([t.numpy() for t in val_df['embedding']])\n",
    "y_val = np.array([int(t) for t in val_df['Year']])\n",
    "X_test = np.array([t.numpy() for t in test_df['embedding']])\n",
    "y_test = np.array([int(t) for t in test_df['Year']])\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_val) == len(y_val))\n",
    "assert(len(X_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "# inilialising the model\n",
    "model = Sequential([\n",
    "    Dense(4096, input_shape = (X_train[0].shape[0], ), activation = \"relu\"),\n",
    "    Dense(1024, activation = \"relu\"),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dense(1, activation = 'linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = Adam(1e-10), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training with 20 epochs and stopping early if validation loss increases for 5 consecutive epochs\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs = 100, \n",
    "         callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110592,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared value for accuracy\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "np.sqrt(mean_squared_error(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [np.mean(y_test)] * len(y_test)\n",
    "np.sqrt(mean_squared_error(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Improvements [5 marks]\n",
    "\n",
    "Propose a possible improvement. Some ideas are suggested below. The chosen improvement must be notified to the lecturer at least 1 week before submission and **must be approved by the lecturer to avoid duplication with other students**. Compare the performance between your original and your new model (the proposed improvement might not actually improve on model performance -- that is ok). Some marks will be awarded for more interesting / challenging / novel improvements.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "* Try a different pretrained model for creating the embedding\n",
    "* Alternative ways of reducing the pretrained model's output to a flat vector for the embedding\n",
    "* Gather more data (see the csv file for URLs)\n",
    "* Add different architectural details to the regression head\n",
    "* Fine-tuning\n",
    "* Training an end-to-end convnet of your own design (no pretraining)\n",
    "* Improve the embedding by training a multi-headed model, eg predicting both genre and year\n",
    "* Create a good visualisation of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "#subsetting image url and year\n",
    "df_5 = df[['imdbId', 'Poster', 'Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all null values and reconfirming\n",
    "df_5 = df_5.dropna()\n",
    "df_5.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "#downloading images and storing them as their respective imdbId.jpg\n",
    "import requests\n",
    "i = 1\n",
    "for img_url, imdbId in zip(df_5['Poster'], df_5['imdbId']):\n",
    "    total = len(df_5)\n",
    "    img_data = requests.get(img_url).content\n",
    "    imdbPresent = []\n",
    "    #only gettings images for status code found\n",
    "    if requests.get(img_url).status_code == 200:\n",
    "        with open(str(imdbId)+'.jpg', 'wb') as handler:\n",
    "            i =  i+1\n",
    "            handler.write(img_data)\n",
    "        imdbPresent.append(imdbId)\n",
    "        sys.stdout.write(\"\\rImages Done: \" + str(i))\n",
    "        sys.stdout.flush()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "Ids = [int(re.sub(\"[^0-9]\", \"\",file)) for file in glob.glob(\"task_images//*.jpg\")]\n",
    "images = [np.array(Image.open(file), dtype=float) for file in glob.glob(\"task_images//*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "for idx in Ids:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_5, train_size=0.7, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Poster',\n",
    "    y_col='Year',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Poster',\n",
    "    y_col='Year',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='Poster',\n",
    "    y_col='Year',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
